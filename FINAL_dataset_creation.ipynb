{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b0277ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "from collections import Counter\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "877f1564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Utility functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Utility Functions\n",
    "\n",
    "def shannon_entropy(text):\n",
    "    \"\"\"Calculate Shannon entropy of a string\"\"\"\n",
    "    if not text or len(text) == 0:\n",
    "        return 0\n",
    "    \n",
    "    text = str(text)\n",
    "    counts = Counter(text)\n",
    "    probs = [count/len(text) for count in counts.values()]\n",
    "    return -sum(p * math.log2(p) for p in probs if p > 0)\n",
    "\n",
    "\n",
    "def extract_domain(url):\n",
    "    \"\"\"Extract domain from URL\"\"\"\n",
    "    try:\n",
    "        parsed = urlparse(str(url))\n",
    "        domain = parsed.netloc if parsed.netloc else parsed.path.split('/')[0]\n",
    "        return domain.lower()\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def count_subdomains(url):\n",
    "    \"\"\"Count number of subdomains\"\"\"\n",
    "    domain = extract_domain(url)\n",
    "    if not domain:\n",
    "        return 0\n",
    "    parts = domain.split('.')\n",
    "    return max(0, len(parts) - 2)\n",
    "\n",
    "\n",
    "def has_ip_address(url):\n",
    "    \"\"\"Check if URL contains IP address\"\"\"\n",
    "    ip_pattern = r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}'\n",
    "    return 1 if re.search(ip_pattern, str(url)) else 0\n",
    "\n",
    "\n",
    "def is_suspicious_tld(tld):\n",
    "    \"\"\"Check if TLD is commonly used for phishing\"\"\"\n",
    "    suspicious_tlds = ['.tk', '.ml', '.ga', '.cf', '.gq', '.xyz', '.top', \n",
    "                       '.work', '.click', '.link', '.download']\n",
    "    return 1 if str(tld).lower() in suspicious_tlds else 0\n",
    "\n",
    "\n",
    "def count_suspicious_chars(url):\n",
    "    \"\"\"Count suspicious characters in URL\"\"\"\n",
    "    suspicious = ['@', '~', '%20', '..', '//']\n",
    "    count = sum(str(url).count(char) for char in suspicious)\n",
    "    return count\n",
    "\n",
    "\n",
    "def extract_brand_keywords(text):\n",
    "    \"\"\"Extract potential brand names from text\"\"\"\n",
    "    brands = ['paypal', 'amazon', 'google', 'microsoft', 'apple', 'facebook',\n",
    "              'netflix', 'bank', 'ebay', 'linkedin', 'instagram', 'twitter']\n",
    "    text_lower = str(text).lower()\n",
    "    found = [brand for brand in brands if brand in text_lower]\n",
    "    return ' '.join(found) if found else \"\"\n",
    "\n",
    "\n",
    "def calculate_domain_trust_score(domain_age, web_who_is, web_https, web_tld):\n",
    "    \"\"\"\n",
    "    Calculate domain trust score based on multiple factors\n",
    "    Score range: 0 (suspicious) to 1 (trusted)\n",
    "    \"\"\"\n",
    "    score = 0.0\n",
    "    \n",
    "    # Age factor (0-0.4)\n",
    "    try:\n",
    "        age = float(domain_age) if domain_age else 0\n",
    "        if age > 365 * 3:\n",
    "            score += 0.4\n",
    "        elif age > 365:\n",
    "            score += 0.3\n",
    "        elif age > 180:\n",
    "            score += 0.2\n",
    "        elif age > 30:\n",
    "            score += 0.1\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # WHOIS completeness (0-0.2)\n",
    "    if str(web_who_is).lower() == 'complete':\n",
    "        score += 0.2\n",
    "    \n",
    "    # HTTPS (0-0.2)\n",
    "    if str(web_https).lower() == 'yes':\n",
    "        score += 0.2\n",
    "    \n",
    "    # TLD reputation (0-0.2)\n",
    "    trusted_tlds = ['.com', '.org', '.net', '.edu', '.gov']\n",
    "    if str(web_tld).lower() in trusted_tlds:\n",
    "        score += 0.2\n",
    "    elif is_suspicious_tld(web_tld):\n",
    "        score -= 0.1\n",
    "    \n",
    "    return max(0, min(1, score))\n",
    "\n",
    "\n",
    "def calculate_semantic_coherence(email_text, url_text, content_text):\n",
    "    \"\"\"\n",
    "    Calculate semantic coherence between email, URL, and content\n",
    "    Returns score 0-1 (1 = highly coherent, 0 = incoherent)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        texts = [str(t).lower() for t in [email_text, url_text, content_text]]\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(max_features=50, stop_words='english')\n",
    "        \n",
    "        non_empty = [t for t in texts if len(t.strip()) > 0]\n",
    "        if len(non_empty) < 2:\n",
    "            return 0.5\n",
    "        \n",
    "        tfidf_matrix = vectorizer.fit_transform(non_empty)\n",
    "        \n",
    "        similarities = []\n",
    "        for i in range(len(non_empty)):\n",
    "            for j in range(i + 1, len(non_empty)):\n",
    "                sim = cosine_similarity(tfidf_matrix[i:i+1], tfidf_matrix[j:j+1])[0][0]\n",
    "                similarities.append(sim)\n",
    "        \n",
    "        return np.mean(similarities) if similarities else 0.5\n",
    "    except:\n",
    "        return 0.5\n",
    "\n",
    "\n",
    "def calculate_email_url_similarity(email_domain, url_domain):\n",
    "    \"\"\"Calculate similarity between email domain and URL domain\"\"\"\n",
    "    if not email_domain or not url_domain:\n",
    "        return 0.0\n",
    "    \n",
    "    email_domain = str(email_domain).lower().replace('www.', '')\n",
    "    url_domain = str(url_domain).lower().replace('www.', '')\n",
    "    \n",
    "    if email_domain == url_domain:\n",
    "        return 1.0\n",
    "    \n",
    "    if email_domain in url_domain or url_domain in email_domain:\n",
    "        return 0.7\n",
    "    \n",
    "    set1 = set(email_domain)\n",
    "    set2 = set(url_domain)\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    \n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "\n",
    "def brand_consistency(email_brand, content_brand):\n",
    "    \"\"\"Check if brands match between email and content\"\"\"\n",
    "    if not email_brand or not content_brand:\n",
    "        return 0.5\n",
    "    \n",
    "    email_set = set(email_brand.split())\n",
    "    content_set = set(content_brand.split())\n",
    "    \n",
    "    if email_set.intersection(content_set):\n",
    "        return 1.0\n",
    "    elif email_set or content_set:\n",
    "        return 0.0\n",
    "    return 0.5\n",
    "\n",
    "print(\"‚úÖ Utility functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13406b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Main function defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Main Feature Engineering Function\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"Add all new features to the dataframe\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"STARTING FEATURE ENGINEERING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # Phase 1: Structural Features\n",
    "    print(\"\\nüìä Phase 1: Structural Features...\")\n",
    "    \n",
    "    df_new['js_obfuscation_ratio'] = df_new['web_js_obf_len'] / (df_new['web_js_len'] + 1)\n",
    "    df_new['url_has_ip'] = df_new['web_url'].apply(has_ip_address)\n",
    "    df_new['url_num_dots'] = df_new['web_url'].apply(lambda x: str(x).count('.'))\n",
    "    df_new['url_num_hyphens'] = df_new['web_url'].apply(lambda x: str(x).count('-'))\n",
    "    df_new['url_num_underscores'] = df_new['web_url'].apply(lambda x: str(x).count('_'))\n",
    "    df_new['url_num_slashes'] = df_new['web_url'].apply(lambda x: str(x).count('/'))\n",
    "    df_new['url_num_queries'] = df_new['web_url'].apply(lambda x: str(x).count('?'))\n",
    "    df_new['url_num_ampersands'] = df_new['web_url'].apply(lambda x: str(x).count('&'))\n",
    "    df_new['url_suspicious_chars'] = df_new['web_url'].apply(count_suspicious_chars)\n",
    "    df_new['domain_num_subdomains'] = df_new['web_url'].apply(count_subdomains)\n",
    "    df_new['domain_contains_numbers'] = df_new['web_url'].apply(\n",
    "        lambda x: 1 if re.search(r'\\d', extract_domain(str(x))) else 0\n",
    "    )\n",
    "    df_new['domain_suspicious_tld'] = df_new['web_tld'].apply(is_suspicious_tld)\n",
    "    \n",
    "    print(\"   ‚úì Added 12 structural features\")\n",
    "    \n",
    "    # Phase 2: Entropy Features\n",
    "    print(\"\\nüî¢ Phase 2: Entropy Features...\")\n",
    "    \n",
    "    df_new['url_entropy'] = df_new['web_url'].apply(shannon_entropy)\n",
    "    df_new['domain_entropy'] = df_new['web_url'].apply(\n",
    "        lambda x: shannon_entropy(extract_domain(str(x)))\n",
    "    )\n",
    "    df_new['content_entropy'] = df_new['web_content'].apply(\n",
    "        lambda x: shannon_entropy(str(x)[:1000])\n",
    "    )\n",
    "    \n",
    "    print(\"   ‚úì Added 3 entropy features\")\n",
    "    \n",
    "    # Phase 3: Domain Trust Score\n",
    "    print(\"\\nüõ°Ô∏è  Phase 3: Domain Trust Score...\")\n",
    "    \n",
    "    df_new['domain_trust_score'] = df_new.apply(\n",
    "        lambda row: calculate_domain_trust_score(\n",
    "            row.get('domain_age', 0),\n",
    "            row.get('web_who_is', ''),\n",
    "            row.get('web_https', ''),\n",
    "            row.get('web_tld', '')\n",
    "        ), axis=1\n",
    "    )\n",
    "    \n",
    "    print(\"   ‚úì Added 1 trust score feature\")\n",
    "\n",
    "    # Phase 4: Email-URL Consistency\n",
    "    print(\"\\nüîó Phase 4: Email-URL Consistency...\")\n",
    "    \n",
    "    df_new['email_domain_extracted'] = df_new['email_from_domain'].apply(extract_domain)\n",
    "    df_new['url_domain_extracted'] = df_new['web_url'].apply(extract_domain)\n",
    "    \n",
    "    df_new['email_domain_matches_url'] = (\n",
    "        df_new['email_domain_extracted'] == df_new['url_domain_extracted']\n",
    "    ).astype(int)\n",
    "    \n",
    "    df_new['email_url_domain_similarity'] = df_new.apply(\n",
    "        lambda row: calculate_email_url_similarity(\n",
    "            row['email_domain_extracted'],\n",
    "            row['url_domain_extracted']\n",
    "        ), axis=1\n",
    "    )\n",
    "    \n",
    "    df_new = df_new.drop(['email_domain_extracted', 'url_domain_extracted'], axis=1)\n",
    "    \n",
    "    print(\"   Added 2 consistency features\")\n",
    "    \n",
    "    # Phase 5: Content Analysis\n",
    "    print(\"\\nüìù Phase 5: Content Analysis...\")\n",
    "    \n",
    "    df_new['content_num_forms'] = df_new['web_content'].apply(\n",
    "        lambda x: str(x).lower().count('<form')\n",
    "    )\n",
    "    df_new['content_num_inputs'] = df_new['web_content'].apply(\n",
    "        lambda x: str(x).lower().count('<input')\n",
    "    )\n",
    "    df_new['content_num_scripts'] = df_new['web_content'].apply(\n",
    "        lambda x: str(x).lower().count('<script')\n",
    "    )\n",
    "    \n",
    "    suspicious_keywords = ['verify', 'urgent', 'suspended', 'account', 'confirm', \n",
    "                          'password', 'update', 'click', 'login', 'security']\n",
    "    df_new['content_suspicious_keywords'] = df_new['web_content'].apply(\n",
    "        lambda x: sum(1 for kw in suspicious_keywords if kw in str(x).lower())\n",
    "    )\n",
    "    \n",
    "    print(\"    Added 4 content features\")\n",
    "    \n",
    "    # Phase 6: Semantic Coherence\n",
    "    print(\"\\nüß† Phase 6: Semantic Coherence...\")\n",
    "    \n",
    "    df_new['semantic_coherence_score'] = df_new.apply(\n",
    "        lambda row: calculate_semantic_coherence(\n",
    "            str(row.get('email_from_domain', '')),\n",
    "            str(row.get('web_url', '')),\n",
    "            str(row.get('web_content', ''))[:500]\n",
    "        ), axis=1\n",
    "    )\n",
    "    \n",
    "    print(\"    Added 1 semantic feature\")\n",
    "    \n",
    "    # Phase 7: Brand Consistency\n",
    "    print(\"\\nüè∑Ô∏è  Phase 7: Brand Consistency...\")\n",
    "    \n",
    "    df_new['email_brand_extracted'] = df_new['email_from_domain'].apply(extract_brand_keywords)\n",
    "    df_new['content_brand_extracted'] = df_new['web_content'].apply(\n",
    "        lambda x: extract_brand_keywords(str(x)[:500])\n",
    "    )\n",
    "    \n",
    "    df_new['brand_consistency_score'] = df_new.apply(\n",
    "        lambda row: brand_consistency(\n",
    "            row['email_brand_extracted'],\n",
    "            row['content_brand_extracted']\n",
    "        ), axis=1\n",
    "    )\n",
    "    \n",
    "    df_new = df_new.drop(['email_brand_extracted', 'content_brand_extracted'], axis=1)\n",
    "    \n",
    "    print(\"    Added 1 brand consistency feature\")\n",
    "    \n",
    "    # Summary\n",
    "    new_features_count = len(df_new.columns) - len(df.columns)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ FEATURE ENGINEERING COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Original features: {len(df.columns)}\")\n",
    "    print(f\"New features added: {new_features_count}\")\n",
    "    print(f\"Total features: {len(df_new.columns)}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "print(\"‚úÖ Main function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "453d0a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Loading dataset...\n",
      "‚úÖ Dataset loaded successfully!\n",
      "Shape: (8000, 15)\n",
      "\n",
      "Columns in original dataset:\n",
      "['email_subject_len', 'email_has_urgent_keyword', 'email_from_domain', 'web_url', 'web_url_len', 'web_ip_add', 'web_geo_loc', 'web_tld', 'web_who_is', 'web_https', 'web_js_len', 'web_js_obf_len', 'web_content', 'domain_age', 'final_label']\n",
      "\n",
      "First 3 rows:\n",
      "   email_subject_len  email_has_urgent_keyword              email_from_domain  \\\n",
      "0                 32                         0  spamassassin.zones.apache.org   \n",
      "1                 46                         0                     gmail.com>   \n",
      "2                 21                         0                telefonica.net>   \n",
      "\n",
      "                              web_url  web_url_len       web_ip_add  \\\n",
      "0  http://tools.ietf.org/html/rfc1583           34     30.180.42.35   \n",
      "1         http://www.quickfixgolf.com           27     150.66.16.42   \n",
      "2           http://www.lvnazarene.org           25  180.123.185.229   \n",
      "\n",
      "     web_geo_loc web_tld web_who_is web_https  web_js_len  web_js_obf_len  \\\n",
      "0  United States     org   complete       yes       137.0             0.0   \n",
      "1          Japan     com   complete       yes        94.0             0.0   \n",
      "2          China     org   complete       yes        44.5             0.0   \n",
      "\n",
      "                                         web_content  domain_age  final_label  \n",
      "0  Conversations, sharing on agriculture and ecol...     11168.0            0  \n",
      "1  Abiola irele virginians also describe a featur...      9692.0            0  \n",
      "2  Wire-guided rocket. tony accardo battle law en...      2344.0            0  \n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load Your Dataset\n",
    "\n",
    "# Load CSV file\n",
    "print(\"\\nüìÇ Loading dataset...\")\n",
    "df = pd.read_csv('new_dataset_classical.csv')  \n",
    "\n",
    "\n",
    "# remove accidental empty columns\n",
    "df = df.loc[:, ~df.columns.str.contains(\"^Unnamed\")]\n",
    "\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColumns in original dataset:\")\n",
    "print(list(df.columns))\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f32f2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RUNNING FEATURE ENGINEERING...\n",
      "============================================================\n",
      "============================================================\n",
      "STARTING FEATURE ENGINEERING\n",
      "============================================================\n",
      "\n",
      "üìä Phase 1: Structural Features...\n",
      "   ‚úì Added 12 structural features\n",
      "\n",
      "üî¢ Phase 2: Entropy Features...\n",
      "   ‚úì Added 3 entropy features\n",
      "\n",
      "üõ°Ô∏è  Phase 3: Domain Trust Score...\n",
      "   ‚úì Added 1 trust score feature\n",
      "\n",
      "üîó Phase 4: Email-URL Consistency...\n",
      "   Added 2 consistency features\n",
      "\n",
      "üìù Phase 5: Content Analysis...\n",
      "    Added 4 content features\n",
      "\n",
      "üß† Phase 6: Semantic Coherence...\n",
      "    Added 1 semantic feature\n",
      "\n",
      "üè∑Ô∏è  Phase 7: Brand Consistency...\n",
      "    Added 1 brand consistency feature\n",
      "\n",
      "============================================================\n",
      "‚úÖ FEATURE ENGINEERING COMPLETE!\n",
      "============================================================\n",
      "Original features: 15\n",
      "New features added: 24\n",
      "Total features: 39\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Run Feature Engineering\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RUNNING FEATURE ENGINEERING...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df_enhanced = engineer_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "855911f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saving FINAL dataset...\n",
      "‚úÖ Saved to: email_phishing_dataset_FINAL.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Save FINAL Dataset\n",
    "\n",
    "\n",
    "print(\"\\nüíæ Saving FINAL dataset...\")\n",
    "output_file = 'email_phishing_dataset_FINAL.csv'\n",
    "df_enhanced.to_csv(output_file, index=False)\n",
    "print(f\"‚úÖ Saved to: {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
