{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ac1637f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Quantum Dataset Generator - Feature Extraction\n",
    "# ========================================\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "import math\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"📦 All libraries imported successfully!\")\n",
    "\n",
    "# Set paths to your dataset files\n",
    "train_data_path = \"train_data.csv\"  # Update with your actual path\n",
    "test_data_path = \"test_data.csv\"    # Update with your actual path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "690a7750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Training set shape: (1200000, 12)\n",
      "Test set shape: (361934, 12)\n",
      "Using 'label' as target column\n",
      "Training target distribution:\n",
      "target\n",
      "good    1172747\n",
      "bad       27253\n",
      "Name: count, dtype: int64\n",
      "Test target distribution:\n",
      "target\n",
      "good    353872\n",
      "bad       8062\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: Load the datasets\n",
    "print(\"Loading datasets...\")\n",
    "train_df = pd.read_csv(train_data_path)\n",
    "test_df = pd.read_csv(test_data_path)\n",
    "\n",
    "print(f\"Training set shape: {train_df.shape}\")\n",
    "print(f\"Test set shape: {test_df.shape}\")\n",
    "\n",
    "# Check if target column exists\n",
    "if 'Label' not in train_df.columns:\n",
    "    # Try to find the target column\n",
    "    target_col = None\n",
    "    for col in train_df.columns:\n",
    "        if col.lower() in ['label', 'target', 'class', 'is_malicious']:\n",
    "            target_col = col\n",
    "            break\n",
    "    \n",
    "    if target_col:\n",
    "        print(f\"Using '{target_col}' as target column\")\n",
    "        train_df = train_df.rename(columns={target_col: 'target'})\n",
    "        test_df = test_df.rename(columns={target_col: 'target'})\n",
    "    else:\n",
    "        # If no target column found, assume the last column is the target\n",
    "        print(\"No explicit target column found, using last column as target\")\n",
    "        train_df = train_df.rename(columns={train_df.columns[-1]: 'target'})\n",
    "        test_df = test_df.rename(columns={test_df.columns[-1]: 'target'})\n",
    "else:\n",
    "    train_df = train_df.rename(columns={'Label': 'target'})\n",
    "    test_df = test_df.rename(columns={'Label': 'target'})\n",
    "\n",
    "print(f\"Training target distribution:\\n{train_df['target'].value_counts()}\")\n",
    "print(f\"Test target distribution:\\n{test_df['target'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb217e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in training set:\n",
      "Unnamed: 0    0\n",
      "url           0\n",
      "url_len       0\n",
      "ip_add        0\n",
      "geo_loc       0\n",
      "tld           0\n",
      "who_is        0\n",
      "https         0\n",
      "js_len        0\n",
      "js_obf_len    0\n",
      "content       0\n",
      "target        0\n",
      "dtype: int64\n",
      "Missing values in test set:\n",
      "Unnamed: 0    0\n",
      "url           0\n",
      "url_len       0\n",
      "ip_add        0\n",
      "geo_loc       0\n",
      "tld           0\n",
      "who_is        0\n",
      "https         0\n",
      "js_len        0\n",
      "js_obf_len    0\n",
      "content       0\n",
      "target        0\n",
      "dtype: int64\n",
      "Training set shape after cleaning: (1200000, 12)\n",
      "Test set shape after cleaning: (361934, 12)\n"
     ]
    }
   ],
   "source": [
    "# CELL 3: Basic preprocessing\n",
    "# Check for missing values\n",
    "print(\"Missing values in training set:\")\n",
    "print(train_df.isnull().sum())\n",
    "print(\"Missing values in test set:\")\n",
    "print(test_df.isnull().sum())\n",
    "\n",
    "# Drop rows with missing values if any\n",
    "train_df = train_df.dropna()\n",
    "test_df = test_df.dropna()\n",
    "\n",
    "print(f\"Training set shape after cleaning: {train_df.shape}\")\n",
    "print(f\"Test set shape after cleaning: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "965c0234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting additional URL features from training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing URLs: 100%|██████████| 1200000/1200000 [00:34<00:00, 35229.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting additional URL features from test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing URLs: 100%|██████████| 361934/361934 [00:09<00:00, 36485.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 16 additional URL-based features\n"
     ]
    }
   ],
   "source": [
    "# CELL 4: Enhanced URL-based feature engineering\n",
    "def extract_url_features(url):\n",
    "    \"\"\"Extract additional features from URL\"\"\"\n",
    "    try:\n",
    "        if not isinstance(url, str):\n",
    "            url = str(url)\n",
    "            \n",
    "        parsed = urlparse(url)\n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        # Additional length features beyond the existing url_length\n",
    "        features['path_length'] = len(parsed.path) if parsed.path else 0\n",
    "        features['query_length'] = len(parsed.query) if parsed.query else 0\n",
    "        features['fragment_length'] = len(parsed.fragment) if parsed.fragment else 0\n",
    "        \n",
    "        # Structural features\n",
    "        features['num_dots'] = url.count('.')\n",
    "        features['num_hyphens'] = url.count('-')\n",
    "        features['num_underscores'] = url.count('_')\n",
    "        features['num_slashes'] = url.count('/')\n",
    "        features['num_questionmarks'] = url.count('?')\n",
    "        features['num_equals'] = url.count('=')\n",
    "        features['num_ampersands'] = url.count('&')\n",
    "        features['num_at_signs'] = url.count('@')\n",
    "        features['num_percent_signs'] = url.count('%')\n",
    "        \n",
    "        # Character composition\n",
    "        features['num_digits'] = sum(1 for c in url if c.isdigit())\n",
    "        features['num_letters'] = sum(1 for c in url if c.isalpha())\n",
    "        features['num_special_chars'] = len(url) - features['num_digits'] - features['num_letters']\n",
    "        \n",
    "        # Entropy features (quantum-inspired)\n",
    "        features['url_entropy'] = calculate_entropy(url)\n",
    "        \n",
    "        return features\n",
    "    except:\n",
    "        # Return default values if URL parsing fails\n",
    "        return {feature: 0 for feature in [\n",
    "            'path_length', 'query_length', 'fragment_length',\n",
    "            'num_dots', 'num_hyphens', 'num_underscores', 'num_slashes',\n",
    "            'num_questionmarks', 'num_equals', 'num_ampersands', \n",
    "            'num_at_signs', 'num_percent_signs', 'num_digits',\n",
    "            'num_letters', 'num_special_chars', 'url_entropy'\n",
    "        ]}\n",
    "\n",
    "def calculate_entropy(text):\n",
    "    \"\"\"Calculate Shannon entropy of a string efficiently\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return 0\n",
    "        \n",
    "    text_length = len(text)\n",
    "    if text_length == 0:\n",
    "        return 0\n",
    "        \n",
    "    # Use numpy for faster entropy calculation\n",
    "    chars, counts = np.unique(list(text), return_counts=True)\n",
    "    probabilities = counts / text_length\n",
    "    entropy = -np.sum(probabilities * np.log2(probabilities + 1e-10))  # Avoid log(0)\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "# Apply URL feature extraction with progress bar\n",
    "print(\"Extracting additional URL features from training set...\")\n",
    "url_features_train = []\n",
    "for url in tqdm(train_df['url'], desc=\"Processing URLs\"):\n",
    "    url_features_train.append(extract_url_features(url))\n",
    "\n",
    "url_features_train = pd.DataFrame(url_features_train)\n",
    "train_df = pd.concat([train_df.reset_index(drop=True), url_features_train], axis=1)\n",
    "\n",
    "print(\"Extracting additional URL features from test set...\")\n",
    "url_features_test = []\n",
    "for url in tqdm(test_df['url'], desc=\"Processing URLs\"):\n",
    "    url_features_test.append(extract_url_features(url))\n",
    "\n",
    "url_features_test = pd.DataFrame(url_features_test)\n",
    "test_df = pd.concat([test_df.reset_index(drop=True), url_features_test], axis=1)\n",
    "\n",
    "print(f\"Added {len(url_features_train.columns)} additional URL-based features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4935b273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting HTML features from training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing HTML: 100%|██████████| 1200000/1200000 [05:18<00:00, 3769.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting HTML features from test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing HTML: 100%|██████████| 361934/361934 [01:36<00:00, 3742.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 14 HTML-based features\n"
     ]
    }
   ],
   "source": [
    "# CELL 5: Enhanced HTML content features\n",
    "# Let's use the exact column names from your dataset\n",
    "url_col = 'url'\n",
    "html_col = 'content'\n",
    "target_col = 'label'\n",
    "\n",
    "def extract_html_features(html_content):\n",
    "    \"\"\"Extract quantum-inspired features from HTML content\"\"\"\n",
    "    if not isinstance(html_content, str):\n",
    "        html_content = str(html_content)\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # Basic HTML structure features\n",
    "    html_length = len(html_content)\n",
    "    features['html_content_length'] = html_length\n",
    "    \n",
    "    # Only process HTML if it's not too long\n",
    "    if html_length < 100000:  # Reasonable limit\n",
    "        features['num_script_tags'] = html_content.count('<script')\n",
    "        features['num_iframe_tags'] = html_content.count('<iframe')\n",
    "        features['num_link_tags'] = html_content.count('<a')\n",
    "        features['num_form_tags'] = html_content.count('<form')\n",
    "        features['num_image_tags'] = html_content.count('<img')\n",
    "        features['num_input_tags'] = html_content.count('<input')\n",
    "        features['num_style_tags'] = html_content.count('<style')\n",
    "        \n",
    "        # Suspicious patterns\n",
    "        features['num_external_links'] = html_content.count('http://') + html_content.count('https://')\n",
    "        features['num_suspicious_keywords'] = sum(1 for keyword in ['eval', 'exec', 'document.write', 'innerHTML', 'fromCharCode'] \n",
    "                                               if keyword in html_content)\n",
    "        \n",
    "        # Quantum-inspired features\n",
    "        features['html_entropy'] = calculate_entropy(html_content) if html_length < 50000 else 0\n",
    "        \n",
    "        # Tag ratios\n",
    "        total_tags = html_content.count('<')\n",
    "        if total_tags > 0:\n",
    "            features['script_ratio'] = features['num_script_tags'] / total_tags\n",
    "            features['iframe_ratio'] = features['num_iframe_tags'] / total_tags\n",
    "            features['external_link_ratio'] = features['num_external_links'] / total_tags\n",
    "        else:\n",
    "            features['script_ratio'] = 0\n",
    "            features['iframe_ratio'] = 0\n",
    "            features['external_link_ratio'] = 0\n",
    "    else:\n",
    "        # Default values for very long HTML\n",
    "        features.update({\n",
    "            'num_script_tags': 0, 'num_iframe_tags': 0, 'num_link_tags': 0,\n",
    "            'num_form_tags': 0, 'num_image_tags': 0, 'num_input_tags': 0,\n",
    "            'num_style_tags': 0, 'num_external_links': 0, 'num_suspicious_keywords': 0,\n",
    "            'html_entropy': 0, 'script_ratio': 0, 'iframe_ratio': 0, 'external_link_ratio': 0\n",
    "        })\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Apply HTML feature extraction with progress bar\n",
    "print(\"Extracting HTML features from training set...\")\n",
    "html_features_train = []\n",
    "for html in tqdm(train_df[html_col], desc=\"Processing HTML\"):\n",
    "    html_features_train.append(extract_html_features(html))\n",
    "\n",
    "html_features_train = pd.DataFrame(html_features_train)\n",
    "train_df = pd.concat([train_df, html_features_train], axis=1)\n",
    "\n",
    "print(\"Extracting HTML features from test set...\")\n",
    "html_features_test = []\n",
    "for html in tqdm(test_df[html_col], desc=\"Processing HTML\"):\n",
    "    html_features_test.append(extract_html_features(html))\n",
    "\n",
    "html_features_test = pd.DataFrame(html_features_test)\n",
    "test_df = pd.concat([test_df, html_features_test], axis=1)\n",
    "\n",
    "print(f\"Added {len(html_features_train.columns)} HTML-based features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb62a332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating quantum-inspired features for training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantum features: 100%|██████████| 1200000/1200000 [01:31<00:00, 13143.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating quantum-inspired features for test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantum features: 100%|██████████| 361934/361934 [00:27<00:00, 13023.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 7 quantum-inspired features\n"
     ]
    }
   ],
   "source": [
    "# CELL 6: Create quantum-inspired features\n",
    "def create_quantum_features(row):\n",
    "    \"\"\"Create features inspired by quantum concepts\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Feature entanglement (correlation-inspired)\n",
    "    features['entanglement_url_html'] = row.get('url_entropy', 0) * row.get('html_entropy', 0)\n",
    "    \n",
    "    # Superposition-inspired features\n",
    "    html_len = row.get('html_content_length', 1)\n",
    "    features['superposition_structure'] = (row.get('num_script_tags', 0) + row.get('num_iframe_tags', 0)) / max(html_len, 1) * 100\n",
    "    \n",
    "    # Quantum probability-inspired features\n",
    "    url_len = max(row.get('url_len', 1), 1)\n",
    "    features['prob_malicious_url'] = min(1.0, row.get('num_special_chars', 0) / url_len * 5)\n",
    "    features['prob_malicious_html'] = min(1.0, (row.get('num_script_tags', 0) + row.get('num_iframe_tags', 0)) / max(html_len, 1) * 100)\n",
    "    \n",
    "    # Quantum state-inspired binary features\n",
    "    features['has_suspicious_elements'] = 1 if (row.get('num_script_tags', 0) > 3 or \n",
    "                                              row.get('num_iframe_tags', 0) > 2 or\n",
    "                                              row.get('num_suspicious_keywords', 0) > 5) else 0\n",
    "    \n",
    "    # JS-related quantum features\n",
    "    js_len = max(row.get('js_len', 1), 1)\n",
    "    js_obf_len = row.get('js_obf_len', 0)\n",
    "    features['js_quantum_ratio'] = js_obf_len / js_len\n",
    "    features['js_entropy_ratio'] = calculate_entropy(str(js_len)) / max(calculate_entropy(str(js_obf_len)), 1)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Apply quantum feature creation with progress bar\n",
    "print(\"Creating quantum-inspired features for training set...\")\n",
    "quantum_features_train = []\n",
    "for _, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"Quantum features\"):\n",
    "    quantum_features_train.append(create_quantum_features(row))\n",
    "\n",
    "quantum_features_train = pd.DataFrame(quantum_features_train)\n",
    "train_df = pd.concat([train_df, quantum_features_train], axis=1)\n",
    "\n",
    "print(\"Creating quantum-inspired features for test set...\")\n",
    "quantum_features_test = []\n",
    "for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Quantum features\"):\n",
    "    quantum_features_test.append(create_quantum_features(row))\n",
    "\n",
    "quantum_features_test = pd.DataFrame(quantum_features_test)\n",
    "test_df = pd.concat([test_df, quantum_features_test], axis=1)\n",
    "\n",
    "print(f\"Added {len(quantum_features_train.columns)} quantum-inspired features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2739325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking what quantum features actually exist in the dataset...\n",
      "Expected quantum features: ['entanglement_url_html', 'superposition_structure', 'prob_malicious_url', 'prob_malicious_html', 'has_suspicious_elements', 'js_quantum_ratio', 'js_entropy_ratio']\n",
      "Available quantum features: ['entanglement_url_html', 'superposition_structure', 'prob_malicious_url', 'prob_malicious_html', 'has_suspicious_elements', 'js_quantum_ratio', 'js_entropy_ratio']\n",
      "Quantum features after recreation: ['entanglement_url_html', 'superposition_structure', 'prob_malicious_url', 'prob_malicious_html', 'has_suspicious_elements', 'js_quantum_ratio', 'js_entropy_ratio']\n",
      "Target variable extracted: y_train shape (1200000,), y_test shape (361934,)\n",
      "\n",
      "Creating quantum-only dataset with features: ['entanglement_url_html', 'superposition_structure', 'prob_malicious_url', 'prob_malicious_html', 'has_suspicious_elements', 'js_quantum_ratio', 'js_entropy_ratio']\n",
      "Quantum training set shape: (1200000, 7)\n",
      "Quantum test set shape: (361934, 7)\n",
      "\n",
      "Missing values in quantum features:\n",
      "entanglement_url_html      0\n",
      "superposition_structure    0\n",
      "prob_malicious_url         0\n",
      "prob_malicious_html        0\n",
      "has_suspicious_elements    0\n",
      "js_quantum_ratio           0\n",
      "js_entropy_ratio           0\n",
      "dtype: int64\n",
      "💾 Quantum-only training dataset saved as 'quantum_only_features_train.csv'\n",
      "💾 Quantum-only test dataset saved as 'quantum_only_features_test.csv'\n",
      "✅ Quantum-only dataset created successfully!\n"
     ]
    }
   ],
   "source": [
    "# CELL: Check what quantum features actually exist in your ORIGINAL dataframe\n",
    "print(\"Checking what quantum features actually exist in the dataset...\")\n",
    "\n",
    "# List all possible quantum features we expect\n",
    "quantum_features_expected = [\n",
    "    'entanglement_url_html', 'superposition_structure',\n",
    "    'prob_malicious_url', 'prob_malicious_html', 'has_suspicious_elements',\n",
    "    'js_quantum_ratio', 'js_entropy_ratio'\n",
    "]\n",
    "\n",
    "# Check which ones actually exist\n",
    "quantum_features_available = [f for f in quantum_features_expected if f in train_df.columns]\n",
    "\n",
    "print(f\"Expected quantum features: {quantum_features_expected}\")\n",
    "print(f\"Available quantum features: {quantum_features_available}\")\n",
    "\n",
    "# Check if any are missing\n",
    "missing_quantum = set(quantum_features_expected) - set(quantum_features_available)\n",
    "if missing_quantum:\n",
    "    print(f\"Missing quantum features: {missing_quantum}\")\n",
    "    \n",
    "    # Let's check why they might be missing by looking at the feature creation code\n",
    "    print(\"\\nChecking if missing features can be recreated...\")\n",
    "    \n",
    "    # Recreate missing quantum features if possible\n",
    "    if 'superposition_structure' in missing_quantum:\n",
    "        print(\"Recreating superposition_structure...\")\n",
    "        train_df['superposition_structure'] = (train_df.get('num_script_tags', 0) + train_df.get('num_iframe_tags', 0)) / np.maximum(train_df.get('html_content_length', 1), 1) * 100\n",
    "        test_df['superposition_structure'] = (test_df.get('num_script_tags', 0) + test_df.get('num_iframe_tags', 0)) / np.maximum(test_df.get('html_content_length', 1), 1) * 100\n",
    "    \n",
    "    if 'prob_malicious_html' in missing_quantum:\n",
    "        print(\"Recreating prob_malicious_html...\")\n",
    "        train_df['prob_malicious_html'] = np.minimum(1.0, (train_df.get('num_script_tags', 0) + train_df.get('num_iframe_tags', 0)) / np.maximum(train_df.get('html_content_length', 1), 1) * 100)\n",
    "        test_df['prob_malicious_html'] = np.minimum(1.0, (test_df.get('num_script_tags', 0) + test_df.get('num_iframe_tags', 0)) / np.maximum(test_df.get('html_content_length', 1), 1) * 100)\n",
    "    \n",
    "    if 'has_suspicious_elements' in missing_quantum:\n",
    "        print(\"Recreating has_suspicious_elements...\")\n",
    "        train_df['has_suspicious_elements'] = ((train_df.get('num_script_tags', 0) > 3) | \n",
    "                                              (train_df.get('num_iframe_tags', 0) > 2) |\n",
    "                                              (train_df.get('num_suspicious_keywords', 0) > 5)).astype(int)\n",
    "        test_df['has_suspicious_elements'] = ((test_df.get('num_script_tags', 0) > 3) | \n",
    "                                             (test_df.get('num_iframe_tags', 0) > 2) |\n",
    "                                             (test_df.get('num_suspicious_keywords', 0) > 5)).astype(int)\n",
    "\n",
    "# Update the available features list\n",
    "quantum_features_available = [f for f in quantum_features_expected if f in train_df.columns]\n",
    "print(f\"Quantum features after recreation: {quantum_features_available}\")\n",
    "\n",
    "# Extract target variables BEFORE creating quantum-only datasets\n",
    "y_train = train_df['target']\n",
    "y_test = test_df['target']\n",
    "\n",
    "print(f\"Target variable extracted: y_train shape {y_train.shape}, y_test shape {y_test.shape}\")\n",
    "\n",
    "# Now create a dataset with ALL quantum features + target\n",
    "quantum_only_features = quantum_features_available\n",
    "print(f\"\\nCreating quantum-only dataset with features: {quantum_only_features}\")\n",
    "\n",
    "# Check if we have the required HTML features for quantum features\n",
    "required_html_features = ['num_script_tags', 'num_iframe_tags', 'html_content_length', 'num_suspicious_keywords']\n",
    "missing_html_for_quantum = [f for f in required_html_features if f not in train_df.columns]\n",
    "\n",
    "if missing_html_for_quantum:\n",
    "    print(f\"Warning: Missing HTML features needed for quantum features: {missing_html_for_quantum}\")\n",
    "    print(\"Some quantum features might not work properly\")\n",
    "\n",
    "# Create quantum-only datasets\n",
    "X_train_quantum = train_df[quantum_only_features]\n",
    "X_test_quantum = test_df[quantum_only_features]\n",
    "\n",
    "print(f\"Quantum training set shape: {X_train_quantum.shape}\")\n",
    "print(f\"Quantum test set shape: {X_test_quantum.shape}\")\n",
    "\n",
    "# Check for missing values in quantum features\n",
    "print(\"\\nMissing values in quantum features:\")\n",
    "print(X_train_quantum.isnull().sum())\n",
    "\n",
    "# Handle any missing values\n",
    "if X_train_quantum.isnull().any().any():\n",
    "    print(\"Filling missing values with 0...\")\n",
    "    X_train_quantum = X_train_quantum.fillna(0)\n",
    "    X_test_quantum = X_test_quantum.fillna(0)\n",
    "\n",
    "# Scale the quantum features\n",
    "scaler_quantum = StandardScaler()\n",
    "X_train_quantum_scaled = scaler_quantum.fit_transform(X_train_quantum)\n",
    "X_test_quantum_scaled = scaler_quantum.transform(X_test_quantum)\n",
    "\n",
    "# Create final quantum dataset\n",
    "X_train_quantum_df = pd.DataFrame(X_train_quantum_scaled, columns=quantum_only_features)\n",
    "X_test_quantum_df = pd.DataFrame(X_test_quantum_scaled, columns=quantum_only_features)\n",
    "\n",
    "# Add target\n",
    "X_train_quantum_df['target'] = y_train.values\n",
    "X_test_quantum_df['target'] = y_test.values\n",
    "\n",
    "# Save quantum-only dataset\n",
    "train_quantum_path = \"quantum_only_features_train.csv\"\n",
    "test_quantum_path = \"quantum_only_features_test.csv\"\n",
    "\n",
    "X_train_quantum_df.to_csv(train_quantum_path, index=False)\n",
    "X_test_quantum_df.to_csv(test_quantum_path, index=False)\n",
    "\n",
    "print(f\"💾 Quantum-only training dataset saved as '{train_quantum_path}'\")\n",
    "print(f\"💾 Quantum-only test dataset saved as '{test_quantum_path}'\")\n",
    "\n",
    "print(\"✅ Quantum-only dataset created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c32dc3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 ANALYZING QUANTUM-ONLY DATASETS\n",
      "==================================================\n",
      "📊 Quantum Training Dataset Shape: (1200000, 8)\n",
      "📊 Quantum Test Dataset Shape: (361934, 8)\n",
      "\n",
      "🎯 QUANTUM FEATURES AVAILABLE:\n",
      "------------------------------\n",
      " 1. entanglement_url_html\n",
      " 2. superposition_structure\n",
      " 3. prob_malicious_url\n",
      " 4. prob_malicious_html\n",
      " 5. has_suspicious_elements\n",
      " 6. js_quantum_ratio\n",
      " 7. js_entropy_ratio\n",
      "\n",
      "📈 QUANTUM FEATURES STATISTICS:\n",
      "-----------------------------------\n",
      "\n",
      "🔹 entanglement_url_html:\n",
      "   Training - Min: -5.0351, Max: 5.2672, Mean: -0.0000\n",
      "   Test     - Min: -4.5443, Max: 4.7738, Mean: -0.0023\n",
      "\n",
      "🔹 superposition_structure:\n",
      "   Training - Min: -1.2905, Max: 10.2113, Mean: -0.0000\n",
      "   Test     - Min: -1.2905, Max: 9.6679, Mean: -0.0003\n",
      "\n",
      "🔹 prob_malicious_url:\n",
      "   Training - Min: -9.0761, Max: 0.6371, Mean: -0.0000\n",
      "   Test     - Min: -8.8884, Max: 0.6371, Mean: -0.0009\n",
      "\n",
      "🔹 prob_malicious_html:\n",
      "   Training - Min: -1.2909, Max: 7.1771, Mean: -0.0000\n",
      "   Test     - Min: -1.2909, Max: 7.1771, Mean: -0.0002\n",
      "\n",
      "🔹 has_suspicious_elements:\n",
      "   Training - Min: -0.5523, Max: 1.8105, Mean: -0.0000\n",
      "   Test     - Min: -0.5523, Max: 1.8105, Mean: -0.0023\n",
      "\n",
      "🔹 js_quantum_ratio:\n",
      "   Training - Min: -0.1398, Max: 9.5661, Mean: 0.0000\n",
      "   Test     - Min: -0.1398, Max: 9.5661, Mean: -0.0013\n",
      "\n",
      "🔹 js_entropy_ratio:\n",
      "   Training - Min: -3.5716, Max: 0.8017, Mean: -0.0000\n",
      "   Test     - Min: -3.5716, Max: 0.8017, Mean: 0.0008\n",
      "\n",
      "🎯 TARGET DISTRIBUTION:\n",
      "-------------------------\n",
      "Training set:\n",
      "target\n",
      "bad       27253\n",
      "good    1172747\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test set:\n",
      "target\n",
      "bad       8062\n",
      "good    353872\n",
      "Name: count, dtype: int64\n",
      "\n",
      "🔍 MISSING VALUES CHECK:\n",
      "-------------------------\n",
      "Training set missing values:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Test set missing values:\n",
      "Series([], dtype: int64)\n",
      "✅ No missing values found!\n",
      "\n",
      "📋 DATA TYPES:\n",
      "---------------\n",
      "entanglement_url_html      float64\n",
      "superposition_structure    float64\n",
      "prob_malicious_url         float64\n",
      "prob_malicious_html        float64\n",
      "has_suspicious_elements    float64\n",
      "js_quantum_ratio           float64\n",
      "js_entropy_ratio           float64\n",
      "target                      object\n",
      "dtype: object\n",
      "\n",
      "📊 CORRELATION WITH TARGET:\n",
      "------------------------------\n",
      "❌ An error occurred: could not convert string to float: 'good'\n",
      "\n",
      "============================================================\n",
      "👀 FIRST 5 ROWS OF QUANTUM TRAINING DATASET:\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entanglement_url_html</th>\n",
       "      <th>superposition_structure</th>\n",
       "      <th>prob_malicious_url</th>\n",
       "      <th>prob_malicious_html</th>\n",
       "      <th>has_suspicious_elements</th>\n",
       "      <th>js_quantum_ratio</th>\n",
       "      <th>js_entropy_ratio</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.168778</td>\n",
       "      <td>-1.290511</td>\n",
       "      <td>-0.869515</td>\n",
       "      <td>-1.290865</td>\n",
       "      <td>-0.552329</td>\n",
       "      <td>-0.139775</td>\n",
       "      <td>0.195356</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.236668</td>\n",
       "      <td>0.094981</td>\n",
       "      <td>0.637054</td>\n",
       "      <td>0.095047</td>\n",
       "      <td>-0.552329</td>\n",
       "      <td>-0.139775</td>\n",
       "      <td>-0.746396</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.084247</td>\n",
       "      <td>-0.649197</td>\n",
       "      <td>0.637054</td>\n",
       "      <td>-0.649356</td>\n",
       "      <td>-0.552329</td>\n",
       "      <td>-0.139775</td>\n",
       "      <td>0.801708</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.251932</td>\n",
       "      <td>-0.060357</td>\n",
       "      <td>0.637054</td>\n",
       "      <td>-0.060337</td>\n",
       "      <td>1.810516</td>\n",
       "      <td>7.500982</td>\n",
       "      <td>-2.012619</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.352408</td>\n",
       "      <td>-0.118024</td>\n",
       "      <td>0.637054</td>\n",
       "      <td>-0.118022</td>\n",
       "      <td>-0.552329</td>\n",
       "      <td>-0.139775</td>\n",
       "      <td>0.195356</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   entanglement_url_html  superposition_structure  prob_malicious_url  \\\n",
       "0              -0.168778                -1.290511           -0.869515   \n",
       "1               0.236668                 0.094981            0.637054   \n",
       "2              -0.084247                -0.649197            0.637054   \n",
       "3              -1.251932                -0.060357            0.637054   \n",
       "4               0.352408                -0.118024            0.637054   \n",
       "\n",
       "   prob_malicious_html  has_suspicious_elements  js_quantum_ratio  \\\n",
       "0            -1.290865                -0.552329         -0.139775   \n",
       "1             0.095047                -0.552329         -0.139775   \n",
       "2            -0.649356                -0.552329         -0.139775   \n",
       "3            -0.060337                 1.810516          7.500982   \n",
       "4            -0.118022                -0.552329         -0.139775   \n",
       "\n",
       "   js_entropy_ratio target  \n",
       "0          0.195356   good  \n",
       "1         -0.746396   good  \n",
       "2          0.801708   good  \n",
       "3         -2.012619    bad  \n",
       "4          0.195356   good  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "📋 QUANTUM FEATURES DESCRIPTION:\n",
      "============================================================\n",
      "• entanglement_url_html     : Quantum entanglement-inspired feature combining URL and HTML entropy\n",
      "• superposition_structure   : Superposition-inspired feature measuring script/iframe density in HTML\n",
      "• prob_malicious_url        : Quantum probability-inspired feature for URL maliciousness likelihood\n",
      "• prob_malicious_html       : Quantum probability-inspired feature for HTML maliciousness likelihood\n",
      "• has_suspicious_elements   : Quantum state binary feature indicating suspicious elements presence\n",
      "• js_quantum_ratio          : JavaScript obfuscation ratio inspired by quantum measurement principles\n",
      "• js_entropy_ratio          : JavaScript entropy ratio inspired by quantum information theory\n"
     ]
    }
   ],
   "source": [
    "# CELL: Analyze Quantum-Only CSV Files\n",
    "print(\"🔬 ANALYZING QUANTUM-ONLY DATASETS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load the quantum-only datasets\n",
    "train_quantum_path = \"quantum_only_features_train.csv\"\n",
    "test_quantum_path = \"quantum_only_features_test.csv\"\n",
    "\n",
    "try:\n",
    "    # Load the datasets\n",
    "    quantum_train = pd.read_csv(train_quantum_path)\n",
    "    quantum_test = pd.read_csv(test_quantum_path)\n",
    "    \n",
    "    print(f\"📊 Quantum Training Dataset Shape: {quantum_train.shape}\")\n",
    "    print(f\"📊 Quantum Test Dataset Shape: {quantum_test.shape}\")\n",
    "    print()\n",
    "    \n",
    "    # Display all columns (quantum features)\n",
    "    print(\"🎯 QUANTUM FEATURES AVAILABLE:\")\n",
    "    print(\"-\" * 30)\n",
    "    for i, col in enumerate(quantum_train.columns, 1):\n",
    "        if col != 'target':\n",
    "            print(f\"{i:2d}. {col}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Feature statistics\n",
    "    print(\"📈 QUANTUM FEATURES STATISTICS:\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    # Get only quantum features (exclude target)\n",
    "    quantum_features = [col for col in quantum_train.columns if col != 'target']\n",
    "    \n",
    "    for feature in quantum_features:\n",
    "        print(f\"\\n🔹 {feature}:\")\n",
    "        print(f\"   Training - Min: {quantum_train[feature].min():.4f}, \"\n",
    "              f\"Max: {quantum_train[feature].max():.4f}, \"\n",
    "              f\"Mean: {quantum_train[feature].mean():.4f}\")\n",
    "        print(f\"   Test     - Min: {quantum_test[feature].min():.4f}, \"\n",
    "              f\"Max: {quantum_test[feature].max():.4f}, \"\n",
    "              f\"Mean: {quantum_test[feature].mean():.4f}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Target distribution\n",
    "    print(\"🎯 TARGET DISTRIBUTION:\")\n",
    "    print(\"-\" * 25)\n",
    "    print(\"Training set:\")\n",
    "    print(quantum_train['target'].value_counts().sort_index())\n",
    "    print(\"\\nTest set:\")\n",
    "    print(quantum_test['target'].value_counts().sort_index())\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Missing values check\n",
    "    print(\"🔍 MISSING VALUES CHECK:\")\n",
    "    print(\"-\" * 25)\n",
    "    print(\"Training set missing values:\")\n",
    "    missing_train = quantum_train.isnull().sum()\n",
    "    print(missing_train[missing_train > 0])\n",
    "    \n",
    "    print(\"\\nTest set missing values:\")\n",
    "    missing_test = quantum_test.isnull().sum()\n",
    "    print(missing_test[missing_test > 0])\n",
    "    \n",
    "    if missing_train.sum() == 0 and missing_test.sum() == 0:\n",
    "        print(\"✅ No missing values found!\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Data types information\n",
    "    print(\"📋 DATA TYPES:\")\n",
    "    print(\"-\" * 15)\n",
    "    print(quantum_train.dtypes)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Correlation with target\n",
    "    print(\"📊 CORRELATION WITH TARGET:\")\n",
    "    print(\"-\" * 30)\n",
    "    correlations = quantum_train.corr()['target'].sort_values(ascending=False)\n",
    "    \n",
    "    # Display correlations (excluding target itself)\n",
    "    correlations = correlations[correlations.index != 'target']\n",
    "    \n",
    "    for feature, corr in correlations.items():\n",
    "        correlation_strength = \"STRONG\" if abs(corr) > 0.5 else \"MODERATE\" if abs(corr) > 0.3 else \"WEAK\"\n",
    "        direction = \"positive\" if corr > 0 else \"negative\"\n",
    "        print(f\"{feature:25} : {corr:7.4f} ({correlation_strength} {direction} correlation)\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Summary\n",
    "    print(\"✅ SUMMARY:\")\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"• Total quantum features: {len(quantum_features)}\")\n",
    "    print(f\"• Training samples: {len(quantum_train):,}\")\n",
    "    print(f\"• Test samples: {len(quantum_test):,}\")\n",
    "    print(f\"• Features with strongest positive correlation: {correlations.head(3).index.tolist()}\")\n",
    "    print(f\"• Features with strongest negative correlation: {correlations.tail(3).index.tolist()}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    print(\"Please make sure the quantum-only CSV files exist in the current directory.\")\n",
    "    print(\"Expected files:\")\n",
    "    print(f\"  - {train_quantum_path}\")\n",
    "    print(f\"  - {test_quantum_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ An error occurred: {e}\")\n",
    "\n",
    "# Additional: Display first few rows for visual inspection\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"👀 FIRST 5 ROWS OF QUANTUM TRAINING DATASET:\")\n",
    "print(\"=\" * 60)\n",
    "try:\n",
    "    display(quantum_train.head())\n",
    "except:\n",
    "    print(quantum_train.head())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"📋 QUANTUM FEATURES DESCRIPTION:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a description of what each quantum feature represents\n",
    "quantum_feature_descriptions = {\n",
    "    'entanglement_url_html': 'Quantum entanglement-inspired feature combining URL and HTML entropy',\n",
    "    'superposition_structure': 'Superposition-inspired feature measuring script/iframe density in HTML',\n",
    "    'prob_malicious_url': 'Quantum probability-inspired feature for URL maliciousness likelihood',\n",
    "    'prob_malicious_html': 'Quantum probability-inspired feature for HTML maliciousness likelihood',\n",
    "    'has_suspicious_elements': 'Quantum state binary feature indicating suspicious elements presence',\n",
    "    'js_quantum_ratio': 'JavaScript obfuscation ratio inspired by quantum measurement principles',\n",
    "    'js_entropy_ratio': 'JavaScript entropy ratio inspired by quantum information theory'\n",
    "}\n",
    "\n",
    "for feature in quantum_features:\n",
    "    description = quantum_feature_descriptions.get(feature, \"No description available\")\n",
    "    print(f\"• {feature:25} : {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62ac852f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'D:\\thesisb1\\Lib\\site-packages\\~klearn'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'D:\\thesisb1\\Lib\\site-packages\\~andas'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'D:\\thesisb1\\Lib\\site-packages\\~atplotlib'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.2 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "✅ Installation complete!\n",
      "🐍 Python version: 3.13\n",
      "✅ Qiskit version: <module 'qiskit.version' from 'd:\\\\thesisb1\\\\Lib\\\\site-packages\\\\qiskit\\\\version.py'>\n",
      "✅ All packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "%pip install -q --upgrade qiskit>=1.2.0 qiskit-algorithms>=0.3.1 qiskit-machine-learning>=0.8.3 qiskit-aer scikit-learn>=1.3.0 pandas>=2.0.0 matplotlib\n",
    "import sys \n",
    "print(f\"✅ Installation complete!\") \n",
    "print(f\"🐍 Python version: {sys.version_info.major}.{sys.version_info.minor}\") \n",
    "# Verify installations \n",
    "try: \n",
    "    import qiskit \n",
    "    import qiskit_algorithms \n",
    "    import qiskit_machine_learning \n",
    "    print(f\"✅ Qiskit version: {qiskit.version}\") \n",
    "    print(f\"✅ All packages installed successfully!\") \n",
    "except ImportError as e: \n",
    "    print(f\"❌ Import error: {e}\") \n",
    "    print(\"🔄 Please restart runtime and try again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "814fe9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔮 QUANTUM SVM TRAINING WITH CORRECTED QISKIT API\n",
      "============================================================\n",
      "📦 All libraries imported successfully!\n",
      "✅ Using corrected FidelityQuantumKernel API\n",
      "📁 Loading quantum-only datasets...\n",
      "📊 Dataset shapes:\n",
      "   Training: (1200000, 7)\n",
      "   Test: (361934, 7)\n",
      "   Features: ['entanglement_url_html', 'superposition_structure', 'prob_malicious_url', 'prob_malicious_html', 'has_suspicious_elements', 'js_quantum_ratio', 'js_entropy_ratio']\n",
      "⚠️  Dataset too large for quantum computation. Sampling 500 samples...\n",
      "🔬 Using 500 training samples and 200 test samples\n",
      "✅ Data preprocessing completed!\n",
      "\n",
      "============================================================\n",
      "🏋️‍♂️ STARTING MODEL TRAINING\n",
      "============================================================\n",
      "\n",
      "🚀 TRAINING BASIC QSVM WITH ZZFeatureMap\n",
      "--------------------------------------------------\n",
      "🔧 Feature Map: ZZFeatureMap with 7 dimensions, 2 repetitions\n",
      "⏳ Training QSVM (this may take a while...)\n",
      "⏳ Making predictions...\n",
      "✅ Basic QSVM Training Completed!\n",
      "📊 Accuracy: 0.9800\n",
      "🎯 Precision: 0.9604\n",
      "🔍 Recall: 0.9800\n",
      "⚖️  F1-Score: 0.9701\n",
      "⏱️  Training Time: 879.35 seconds\n",
      "\n",
      "🚀 TRAINING ADVANCED QSVM WITH ENHANCED FEATURE MAP\n",
      "--------------------------------------------------\n",
      "🔧 Feature Map: Enhanced ZZFeatureMap with 7 dimensions, 2 repetitions, linear entanglement\n",
      "⏳ Training Advanced QSVM...\n",
      "✅ Advanced QSVM Training Completed!\n",
      "📊 Accuracy: 0.9800\n",
      "🎯 Precision: 0.9604\n",
      "🔍 Recall: 0.9800\n",
      "⚖️  F1-Score: 0.9701\n",
      "⏱️  Training Time: 950.59 seconds\n",
      "\n",
      "🚀 TRAINING CLASSICAL SVM FOR COMPARISON\n",
      "--------------------------------------------------\n",
      "✅ Classical SVM Training Completed!\n",
      "📊 Accuracy: 0.9950\n",
      "🎯 Precision: 0.9950\n",
      "🔍 Recall: 0.9950\n",
      "⚖️  F1-Score: 0.9946\n",
      "⏱️  Training Time: 0.01 seconds\n",
      "\n",
      "============================================================\n",
      "📊 COMPREHENSIVE RESULTS COMPARISON\n",
      "============================================================\n",
      "Model           Accuracy   Training Time (s) Status    \n",
      "-------------------------------------------------------\n",
      "Basic QSVM      0.9800     879.35          ✅ Success \n",
      "Advanced QSVM   0.9800     950.59          ✅ Success \n",
      "Classical SVM   0.9950     0.01            ✅ Success \n",
      "\n",
      "🏆 BEST MODEL: Classical SVM with accuracy 0.9950\n",
      "\n",
      "============================================================\n",
      "🔍 DETAILED EVALUATION OF BEST MODEL\n",
      "============================================================\n",
      "📈 Detailed results for Classical SVM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       1.00      0.75      0.86         4\n",
      "        good       0.99      1.00      1.00       196\n",
      "\n",
      "    accuracy                           0.99       200\n",
      "   macro avg       1.00      0.88      0.93       200\n",
      "weighted avg       1.00      0.99      0.99       200\n",
      "\n",
      "📋 Confusion Matrix:\n",
      "[[  3   1]\n",
      " [  0 196]]\n",
      "\n",
      "🎯 Quantum Feature Analysis:\n",
      "Features used: ['entanglement_url_html', 'superposition_structure', 'prob_malicious_url', 'prob_malicious_html', 'has_suspicious_elements', 'js_quantum_ratio', 'js_entropy_ratio']\n",
      "\n",
      "💾 SAVING TRAINED MODELS\n",
      "-------------------------\n",
      "✅ Basic QSVC model saved as 'qsvc_basic_model.pkl'\n",
      "✅ Advanced QSVC model saved as 'qsvc_advanced_model.pkl'\n",
      "✅ Classical SVM model saved as 'classical_svm_model.pkl'\n",
      "✅ Feature scaler saved as 'quantum_feature_scaler.pkl'\n",
      "✅ Model metadata saved as 'model_metadata.pkl'\n",
      "\n",
      "✅ QUANTUM SVM TRAINING COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "\n",
      "💡 RECOMMENDATIONS FOR QUANTUM ML:\n",
      "----------------------------------------\n",
      "• Start with small datasets for quantum experiments\n",
      "• Use proper feature scaling for quantum circuits\n",
      "• Experiment with different feature map repetitions\n",
      "• Consider hybrid quantum-classical approaches for larger datasets\n",
      "• Monitor training time vs. accuracy trade-offs\n",
      "\n",
      "🎯 Next steps: Use the saved 'Classical SVM' model for predictions\n"
     ]
    }
   ],
   "source": [
    "# CELL: Quantum SVM Training with Corrected Qiskit API\n",
    "print(\"🔮 QUANTUM SVM TRAINING WITH CORRECTED QISKIT API\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Qiskit imports\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes\n",
    "from qiskit_algorithms.utils import algorithm_globals\n",
    "\n",
    "# CORRECTED: Updated FidelityQuantumKernel import and usage\n",
    "from qiskit_machine_learning.kernels import FidelityQuantumKernel\n",
    "from qiskit_machine_learning.algorithms import QSVC\n",
    "from qiskit.primitives import Sampler\n",
    "from qiskit_aer import AerSimulator\n",
    "\n",
    "print(\"📦 All libraries imported successfully!\")\n",
    "print(\"✅ Using corrected FidelityQuantumKernel API\")\n",
    "\n",
    "# Additional imports for evaluation\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import time\n",
    "import joblib\n",
    "\n",
    "# Load the quantum-only datasets\n",
    "print(\"📁 Loading quantum-only datasets...\")\n",
    "quantum_train = pd.read_csv(\"quantum_only_features_train.csv\")\n",
    "quantum_test = pd.read_csv(\"quantum_only_features_test.csv\")\n",
    "\n",
    "# Prepare features and targets\n",
    "X_train = quantum_train.drop('target', axis=1).values\n",
    "y_train = quantum_train['target'].values\n",
    "X_test = quantum_test.drop('target', axis=1).values\n",
    "y_test = quantum_test['target'].values\n",
    "\n",
    "print(f\"📊 Dataset shapes:\")\n",
    "print(f\"   Training: {X_train.shape}\")\n",
    "print(f\"   Test: {X_test.shape}\")\n",
    "print(f\"   Features: {quantum_train.drop('target', axis=1).columns.tolist()}\")\n",
    "\n",
    "# Reduce dataset size for quantum computation\n",
    "MAX_SAMPLES = 500  # Reduced for faster execution\n",
    "MAX_TEST_SAMPLES = 200\n",
    "\n",
    "if len(X_train) > MAX_SAMPLES:\n",
    "    print(f\"⚠️  Dataset too large for quantum computation. Sampling {MAX_SAMPLES} samples...\")\n",
    "    indices = np.random.choice(len(X_train), MAX_SAMPLES, replace=False)\n",
    "    X_train_small = X_train[indices]\n",
    "    y_train_small = y_train[indices]\n",
    "else:\n",
    "    X_train_small = X_train\n",
    "    y_train_small = y_train\n",
    "\n",
    "if len(X_test) > MAX_TEST_SAMPLES:\n",
    "    indices_test = np.random.choice(len(X_test), MAX_TEST_SAMPLES, replace=False)\n",
    "    X_test_small = X_test[indices_test]\n",
    "    y_test_small = y_test[indices_test]\n",
    "else:\n",
    "    X_test_small = X_test\n",
    "    y_test_small = y_test\n",
    "\n",
    "print(f\"🔬 Using {len(X_train_small)} training samples and {len(X_test_small)} test samples\")\n",
    "\n",
    "# Normalize features for quantum computation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_train_scaled = scaler.fit_transform(X_train_small)\n",
    "X_test_scaled = scaler.transform(X_test_small)\n",
    "\n",
    "print(\"✅ Data preprocessing completed!\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "algorithm_globals.random_seed = 42\n",
    "\n",
    "# METHOD 1: Basic QSVM with ZZFeatureMap (CORRECTED)\n",
    "def train_basic_qsvm(X_train, X_test, y_train, y_test):\n",
    "    print(\"\\n🚀 TRAINING BASIC QSVM WITH ZZFeatureMap\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create feature map\n",
    "    feature_dim = X_train.shape[1]\n",
    "    feature_map = ZZFeatureMap(feature_dimension=feature_dim, reps=2, entanglement='linear')\n",
    "    \n",
    "    print(f\"🔧 Feature Map: ZZFeatureMap with {feature_dim} dimensions, 2 repetitions\")\n",
    "    \n",
    "    # CORRECTED: Create fidelity quantum kernel with proper parameters\n",
    "    try:\n",
    "        # Try the new API first\n",
    "        quantum_kernel = FidelityQuantumKernel(\n",
    "            feature_map=feature_map,\n",
    "            fidelity=None,  # Let it use default\n",
    "            enforce_psd=True\n",
    "        )\n",
    "    except TypeError:\n",
    "        # Fallback to simpler initialization\n",
    "        quantum_kernel = FidelityQuantumKernel(feature_map=feature_map)\n",
    "    \n",
    "    # Create QSVC (Quantum Support Vector Classifier)\n",
    "    qsvc = QSVC(quantum_kernel=quantum_kernel)\n",
    "    \n",
    "    try:\n",
    "        # Train the model\n",
    "        print(\"⏳ Training QSVM (this may take a while...)\")\n",
    "        qsvc.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        print(\"⏳ Making predictions...\")\n",
    "        y_pred = qsvc.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"✅ Basic QSVM Training Completed!\")\n",
    "        print(f\"📊 Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"🎯 Precision: {precision:.4f}\")\n",
    "        print(f\"🔍 Recall: {recall:.4f}\")\n",
    "        print(f\"⚖️  F1-Score: {f1:.4f}\")\n",
    "        print(f\"⏱️  Training Time: {training_time:.2f} seconds\")\n",
    "        \n",
    "        return qsvc, y_pred, accuracy, training_time, feature_map\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Basic QSVM failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, 0, 0, None\n",
    "\n",
    "# METHOD 2: Advanced QSVM with Custom Parameters (CORRECTED)\n",
    "def train_advanced_qsvm(X_train, X_test, y_train, y_test):\n",
    "    print(\"\\n🚀 TRAINING ADVANCED QSVM WITH ENHANCED FEATURE MAP\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create enhanced feature map\n",
    "    feature_dim = X_train.shape[1]\n",
    "    \n",
    "    # Use different parameters for better performance\n",
    "    feature_map = ZZFeatureMap(\n",
    "        feature_dimension=feature_dim, \n",
    "        reps=2,  # Reduced for stability\n",
    "        entanglement='linear'  # Changed to linear for stability\n",
    "    )\n",
    "    \n",
    "    print(f\"🔧 Feature Map: Enhanced ZZFeatureMap with {feature_dim} dimensions, 2 repetitions, linear entanglement\")\n",
    "    \n",
    "    # CORRECTED: Create quantum kernel\n",
    "    try:\n",
    "        quantum_kernel = FidelityQuantumKernel(\n",
    "            feature_map=feature_map,\n",
    "            fidelity=None,\n",
    "            enforce_psd=True\n",
    "        )\n",
    "    except TypeError:\n",
    "        quantum_kernel = FidelityQuantumKernel(feature_map=feature_map)\n",
    "    \n",
    "    # Create QSVC with potential parameter tuning\n",
    "    qsvc_advanced = QSVC(quantum_kernel=quantum_kernel)\n",
    "    \n",
    "    try:\n",
    "        print(\"⏳ Training Advanced QSVM...\")\n",
    "        qsvc_advanced.fit(X_train, y_train)\n",
    "        y_pred_advanced = qsvc_advanced.predict(X_test)\n",
    "        \n",
    "        accuracy_advanced = accuracy_score(y_test, y_pred_advanced)\n",
    "        precision_advanced = precision_score(y_test, y_pred_advanced, average='weighted', zero_division=0)\n",
    "        recall_advanced = recall_score(y_test, y_pred_advanced, average='weighted', zero_division=0)\n",
    "        f1_advanced = f1_score(y_test, y_pred_advanced, average='weighted', zero_division=0)\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"✅ Advanced QSVM Training Completed!\")\n",
    "        print(f\"📊 Accuracy: {accuracy_advanced:.4f}\")\n",
    "        print(f\"🎯 Precision: {precision_advanced:.4f}\")\n",
    "        print(f\"🔍 Recall: {recall_advanced:.4f}\")\n",
    "        print(f\"⚖️  F1-Score: {f1_advanced:.4f}\")\n",
    "        print(f\"⏱️  Training Time: {training_time:.2f} seconds\")\n",
    "        \n",
    "        return qsvc_advanced, y_pred_advanced, accuracy_advanced, training_time, feature_map\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Advanced QSVM failed: {e}\")\n",
    "        return None, None, 0, 0, None\n",
    "\n",
    "# METHOD 3: Classical SVM for comparison\n",
    "def train_classical_svm(X_train, X_test, y_train, y_test):\n",
    "    print(\"\\n🚀 TRAINING CLASSICAL SVM FOR COMPARISON\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Use RBF kernel which is similar to quantum kernels\n",
    "    svm_classical = SVC(kernel='rbf', gamma='scale', random_state=42)\n",
    "    svm_classical.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_classical = svm_classical.predict(X_test)\n",
    "    accuracy_classical = accuracy_score(y_test, y_pred_classical)\n",
    "    precision_classical = precision_score(y_test, y_pred_classical, average='weighted', zero_division=0)\n",
    "    recall_classical = recall_score(y_test, y_pred_classical, average='weighted', zero_division=0)\n",
    "    f1_classical = f1_score(y_test, y_pred_classical, average='weighted', zero_division=0)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"✅ Classical SVM Training Completed!\")\n",
    "    print(f\"📊 Accuracy: {accuracy_classical:.4f}\")\n",
    "    print(f\"🎯 Precision: {precision_classical:.4f}\")\n",
    "    print(f\"🔍 Recall: {recall_classical:.4f}\")\n",
    "    print(f\"⚖️  F1-Score: {f1_classical:.4f}\")\n",
    "    print(f\"⏱️  Training Time: {training_time:.2f} seconds\")\n",
    "    \n",
    "    return svm_classical, y_pred_classical, accuracy_classical, training_time\n",
    "\n",
    "# Train all models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🏋️‍♂️ STARTING MODEL TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train Basic QSVM\n",
    "qsvc_basic, y_pred_basic, acc_basic, time_basic, feature_map_basic = train_basic_qsvm(\n",
    "    X_train_scaled, X_test_scaled, y_train_small, y_test_small\n",
    ")\n",
    "\n",
    "# Train Advanced QSVM\n",
    "qsvc_advanced, y_pred_advanced, acc_advanced, time_advanced, feature_map_advanced = train_advanced_qsvm(\n",
    "    X_train_scaled, X_test_scaled, y_train_small, y_test_small\n",
    ")\n",
    "\n",
    "# Train Classical SVM\n",
    "svm_classical, y_pred_classical, acc_classical, time_classical = train_classical_svm(\n",
    "    X_train_scaled, X_test_scaled, y_train_small, y_test_small\n",
    ")\n",
    "\n",
    "# Results Comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📊 COMPREHENSIVE RESULTS COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = {\n",
    "    'Basic QSVM': (acc_basic, time_basic, y_pred_basic if qsvc_basic is not None else None),\n",
    "    'Advanced QSVM': (acc_advanced, time_advanced, y_pred_advanced if qsvc_advanced is not None else None),\n",
    "    'Classical SVM': (acc_classical, time_classical, y_pred_classical)\n",
    "}\n",
    "\n",
    "print(f\"{'Model':<15} {'Accuracy':<10} {'Training Time (s)':<15} {'Status':<10}\")\n",
    "print(\"-\" * 55)\n",
    "for model, (acc, t_time, preds) in results.items():\n",
    "    status = \"✅ Success\" if preds is not None else \"❌ Failed\"\n",
    "    if preds is not None:\n",
    "        print(f\"{model:<15} {acc:<10.4f} {t_time:<15.2f} {status:<10}\")\n",
    "    else:\n",
    "        print(f\"{model:<15} {'N/A':<10} {t_time:<15.2f} {status:<10}\")\n",
    "\n",
    "# Find best model\n",
    "successful_models = {k: v for k, v in results.items() if v[2] is not None}\n",
    "if successful_models:\n",
    "    best_model_name = max(successful_models.items(), key=lambda x: x[1][0])[0]\n",
    "    best_accuracy = successful_models[best_model_name][0]\n",
    "    print(f\"\\n🏆 BEST MODEL: {best_model_name} with accuracy {best_accuracy:.4f}\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  No quantum models succeeded. Using classical SVM as fallback.\")\n",
    "    best_model_name = \"Classical SVM\"\n",
    "    best_accuracy = acc_classical\n",
    "\n",
    "# Detailed evaluation for the best successful model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🔍 DETAILED EVALUATION OF BEST MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if best_model_name == \"Basic QSVM\" and qsvc_basic is not None:\n",
    "    best_model = qsvc_basic\n",
    "    best_y_pred = y_pred_basic\n",
    "    feature_map = feature_map_basic\n",
    "elif best_model_name == \"Advanced QSVM\" and qsvc_advanced is not None:\n",
    "    best_model = qsvc_advanced\n",
    "    best_y_pred = y_pred_advanced\n",
    "    feature_map = feature_map_advanced\n",
    "else:\n",
    "    best_model = svm_classical\n",
    "    best_y_pred = y_pred_classical\n",
    "    feature_map = None\n",
    "\n",
    "print(f\"📈 Detailed results for {best_model_name}:\")\n",
    "print(classification_report(y_test_small, best_y_pred, zero_division=0))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test_small, best_y_pred)\n",
    "print(\"📋 Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Feature importance analysis\n",
    "print(\"\\n🎯 Quantum Feature Analysis:\")\n",
    "feature_names = quantum_train.drop('target', axis=1).columns.tolist()\n",
    "print(\"Features used:\", feature_names)\n",
    "\n",
    "# Quantum Circuit Information\n",
    "if best_model_name != \"Classical SVM\" and feature_map is not None:\n",
    "    print(\"\\n🔬 QUANTUM CIRCUIT INFORMATION\")\n",
    "    print(\"-\" * 35)\n",
    "    try:\n",
    "        print(f\"Quantum circuit depth: {feature_map.decompose().depth()}\")\n",
    "        print(f\"Number of quantum gates: {len(feature_map.decompose().data)}\")\n",
    "        print(f\"Number of qubits: {feature_map.num_qubits}\")\n",
    "        \n",
    "        # Display the feature map circuit\n",
    "        print(\"\\n📊 Feature Map Circuit:\")\n",
    "        print(feature_map.decompose().draw(output='text'))\n",
    "    except Exception as e:\n",
    "        print(f\"Circuit analysis error: {e}\")\n",
    "\n",
    "# Save the trained models\n",
    "print(\"\\n💾 SAVING TRAINED MODELS\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "if qsvc_basic is not None:\n",
    "    joblib.dump(qsvc_basic, 'qsvc_basic_model.pkl')\n",
    "    print(\"✅ Basic QSVC model saved as 'qsvc_basic_model.pkl'\")\n",
    "\n",
    "if qsvc_advanced is not None:\n",
    "    joblib.dump(qsvc_advanced, 'qsvc_advanced_model.pkl')\n",
    "    print(\"✅ Advanced QSVC model saved as 'qsvc_advanced_model.pkl'\")\n",
    "\n",
    "joblib.dump(svm_classical, 'classical_svm_model.pkl')\n",
    "print(\"✅ Classical SVM model saved as 'classical_svm_model.pkl'\")\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, 'quantum_feature_scaler.pkl')\n",
    "print(\"✅ Feature scaler saved as 'quantum_feature_scaler.pkl'\")\n",
    "\n",
    "# Save feature names\n",
    "feature_info = {\n",
    "    'feature_names': feature_names,\n",
    "    'feature_count': len(feature_names),\n",
    "    'best_model': best_model_name,\n",
    "    'best_accuracy': best_accuracy\n",
    "}\n",
    "joblib.dump(feature_info, 'model_metadata.pkl')\n",
    "print(\"✅ Model metadata saved as 'model_metadata.pkl'\")\n",
    "\n",
    "print(\"\\n✅ QUANTUM SVM TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Final recommendations\n",
    "print(\"\\n💡 RECOMMENDATIONS FOR QUANTUM ML:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"• Start with small datasets for quantum experiments\")\n",
    "print(\"• Use proper feature scaling for quantum circuits\")\n",
    "print(\"• Experiment with different feature map repetitions\")\n",
    "print(\"• Consider hybrid quantum-classical approaches for larger datasets\")\n",
    "print(\"• Monitor training time vs. accuracy trade-offs\")\n",
    "\n",
    "print(f\"\\n🎯 Next steps: Use the saved '{best_model_name}' model for predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8993a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📊 COMPLETE RESULTS SUMMARY\n",
      "================================================================================\n",
      "Model                Accuracy   Precision  Recall     F1-Score   Time (s)     Status    \n",
      "-------------------------------------------------------------------------------------\n",
      "Basic QSVM           0.9800     0.9604     0.9800     0.9701     879.35       ✅ Success \n",
      "Advanced QSVM        0.9800     0.9604     0.9800     0.9701     950.59       ✅ Success \n",
      "Classical SVM        0.9950     0.9950     0.9950     0.9946     0.01         ✅ Success \n",
      "\n",
      "🏆 ACTUAL BEST MODEL: Classical SVM with accuracy 0.9950\n"
     ]
    }
   ],
   "source": [
    "# CELL: Display Complete Results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📊 COMPLETE RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Re-display results with full details\n",
    "results = {\n",
    "    'Basic QSVM': (acc_basic, time_basic, y_pred_basic if qsvc_basic is not None else None),\n",
    "    'Advanced QSVM': (acc_advanced, time_advanced, y_pred_advanced if qsvc_advanced is not None else None),\n",
    "    'Classical SVM': (acc_classical, time_classical, y_pred_classical)\n",
    "}\n",
    "\n",
    "print(f\"{'Model':<20} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Time (s)':<12} {'Status':<10}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "for model, (acc, t_time, preds) in results.items():\n",
    "    status = \"✅ Success\" if preds is not None else \"❌ Failed\"\n",
    "    if preds is not None:\n",
    "        # Calculate metrics for this model\n",
    "        precision = precision_score(y_test_small, preds, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_test_small, preds, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_test_small, preds, average='weighted', zero_division=0)\n",
    "        \n",
    "        print(f\"{model:<20} {acc:<10.4f} {precision:<10.4f} {recall:<10.4f} {f1:<10.4f} {t_time:<12.2f} {status:<10}\")\n",
    "    else:\n",
    "        print(f\"{model:<20} {'N/A':<10} {'N/A':<10} {'N/A':<10} {'N/A':<10} {t_time:<12.2f} {status:<10}\")\n",
    "\n",
    "# Show which model actually performed best\n",
    "successful_models = {k: v for k, v in results.items() if v[2] is not None}\n",
    "if successful_models:\n",
    "    best_model_name = max(successful_models.items(), key=lambda x: x[1][0])[0]\n",
    "    best_accuracy = successful_models[best_model_name][0]\n",
    "    print(f\"\\n🏆 ACTUAL BEST MODEL: {best_model_name} with accuracy {best_accuracy:.4f}\")\n",
    "    \n",
    "    # Show why Classical SVM was chosen if it wasn't the best\n",
    "    if best_model_name != \"Classical SVM\":\n",
    "        classical_acc = results['Classical SVM'][0]\n",
    "        print(f\"💡 Note: Classical SVM accuracy was {classical_acc:.4f}\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  No quantum models succeeded. Classical SVM accuracy: {acc_classical:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5b7dae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "🔍 INDIVIDUAL MODEL STATUS CHECK\n",
      "============================================================\n",
      "Basic QSVM Status: ✅ Trained\n",
      "  - Accuracy: 0.9800\n",
      "  - Training Time: 879.35s\n",
      "Advanced QSVM Status: ✅ Trained\n",
      "  - Accuracy: 0.9800\n",
      "  - Training Time: 950.59s\n",
      "Classical SVM Status: ✅ Trained\n",
      "  - Accuracy: 0.9950\n",
      "  - Training Time: 0.01s\n"
     ]
    }
   ],
   "source": [
    "# CELL: Debug Individual Model Results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🔍 INDIVIDUAL MODEL STATUS CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Basic QSVM Status: {'✅ Trained' if qsvc_basic is not None else '❌ Failed'}\")\n",
    "if qsvc_basic is not None:\n",
    "    print(f\"  - Accuracy: {acc_basic:.4f}\")\n",
    "    print(f\"  - Training Time: {time_basic:.2f}s\")\n",
    "\n",
    "print(f\"Advanced QSVM Status: {'✅ Trained' if qsvc_advanced is not None else '❌ Failed'}\")\n",
    "if qsvc_advanced is not None:\n",
    "    print(f\"  - Accuracy: {acc_advanced:.4f}\")\n",
    "    print(f\"  - Training Time: {time_advanced:.2f}s\")\n",
    "\n",
    "print(f\"Classical SVM Status: ✅ Trained\")\n",
    "print(f\"  - Accuracy: {acc_classical:.4f}\")\n",
    "print(f\"  - Training Time: {time_classical:.2f}s\")\n",
    "\n",
    "# Check if Advanced QSVM failed\n",
    "if qsvc_advanced is None:\n",
    "    print(\"\\n❌ Advanced QSVM failed to train. Possible reasons:\")\n",
    "    print(\"   - Too complex feature map for the dataset size\")\n",
    "    print(\"   - Memory issues with full entanglement\")\n",
    "    print(\"   - Numerical instability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8c79cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 15 available features\n",
      "Training feature matrix shape: (1200000, 15)\n",
      "Test feature matrix shape: (361934, 15)\n",
      "Converted 'good'->0, 'bad'->1\n",
      "Target values after conversion: [0 1]\n"
     ]
    }
   ],
   "source": [
    "# CELL 7: Prepare the final dataset (FIXED)\n",
    "# Select features for the final dataset\n",
    "target_col = 'target'\n",
    "feature_columns = [\n",
    "    # Original features from your dataset\n",
    "    'url_len', 'ip_add', 'geo_loc', 'tld', 'who_is', 'https', \n",
    "    'js_len', 'js_obf_len',\n",
    "    \n",
    "    # Enhanced URL features\n",
    "    'path_length', 'query_length', 'fragment_length',\n",
    "    'num_dots', 'num_hyphens', 'num_underscores', 'num_slashes',\n",
    "    'num_questionmarks', 'num_equals', 'num_ampersands', \n",
    "    'num_at_signs', 'num_percent_signs', 'num_digits',\n",
    "    'num_letters', 'num_special_chars', 'url_entropy',\n",
    "    \n",
    "    # Enhanced HTML features\n",
    "    'html_content_length', 'num_script_tags', 'num_iframe_tags', \n",
    "    'num_link_tags', 'num_form_tags', 'num_image_tags',\n",
    "    'num_input_tags', 'num_style_tags', 'num_external_links',\n",
    "    'num_suspicious_keywords', 'html_entropy', 'script_ratio',\n",
    "    'iframe_ratio', 'external_link_ratio',\n",
    "    \n",
    "    # Quantum features\n",
    "    'entanglement_url_html', 'superposition_structure',\n",
    "    'prob_malicious_url', 'prob_malicious_html', 'has_suspicious_elements',\n",
    "    'js_quantum_ratio', 'js_entropy_ratio'\n",
    "]\n",
    "\n",
    "# Keep only features that actually exist in the dataframe\n",
    "available_features = [f for f in feature_columns if f in train_df.columns]\n",
    "print(f\"Using {len(available_features)} available features\")\n",
    "\n",
    "# Create the feature datasets\n",
    "X_train = train_df[available_features]\n",
    "y_train = train_df[target_col]  # Use the correct target column\n",
    "X_test = test_df[available_features]\n",
    "y_test = test_df[target_col]    # Use the correct target column\n",
    "\n",
    "print(f\"Training feature matrix shape: {X_train.shape}\")\n",
    "print(f\"Test feature matrix shape: {X_test.shape}\")\n",
    "\n",
    "# Convert target to binary (good=0, bad=1)\n",
    "if y_train.dtype == 'object':\n",
    "    y_train = y_train.map({'good': 0, 'bad': 1})\n",
    "    y_test = y_test.map({'good': 0, 'bad': 1})\n",
    "    print(\"Converted 'good'->0, 'bad'->1\")\n",
    "\n",
    "print(f\"Target values after conversion: {np.unique(y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c7dac60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking data types before scaling...\n",
      "url_len                    int64\n",
      "ip_add                    object\n",
      "geo_loc                   object\n",
      "tld                       object\n",
      "https                     object\n",
      "js_len                   float64\n",
      "js_obf_len               float64\n",
      "url_entropy              float64\n",
      "html_content_length        int64\n",
      "num_script_tags            int64\n",
      "script_ratio             float64\n",
      "entanglement_url_html    float64\n",
      "prob_malicious_url       float64\n",
      "js_quantum_ratio         float64\n",
      "js_entropy_ratio         float64\n",
      "dtype: object\n",
      "Non-numeric columns: ['ip_add', 'geo_loc', 'tld', 'https']\n",
      "Numeric columns: ['url_len', 'js_len', 'js_obf_len', 'url_entropy', 'html_content_length', 'num_script_tags', 'script_ratio', 'entanglement_url_html', 'prob_malicious_url', 'js_quantum_ratio', 'js_entropy_ratio']\n",
      "Processing non-numeric columns...\n",
      "Converting IP addresses to numeric format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_19152\\3602056408.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['ip_add_numeric'] = X_train['ip_add'].apply(ip_to_numeric)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_19152\\3602056408.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test['ip_add_numeric'] = X_test['ip_add'].apply(ip_to_numeric)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding categorical columns: ['geo_loc', 'tld', 'https']\n",
      "  Skipping geo_loc (too many categories: 234)\n",
      "  Skipping tld (too many categories: 1246)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_19152\\3602056408.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col + '_encoded'] = le.fit_transform(X_train[col].astype(str))\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_19152\\3602056408.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col + '_encoded'] = le.transform(X_test[col].astype(str))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Encoded https with 2 categories\n",
      "Final numeric columns for scaling: ['url_len', 'js_len', 'js_obf_len', 'url_entropy', 'html_content_length', 'num_script_tags', 'script_ratio', 'entanglement_url_html', 'prob_malicious_url', 'js_quantum_ratio', 'js_entropy_ratio', 'ip_add_numeric', 'https_encoded']\n",
      "Final training set shape: (1200000, 14)\n",
      "Final test set shape: (361934, 14)\n"
     ]
    }
   ],
   "source": [
    "# CELL 8: Data normalization (FIXED)\n",
    "print(\"Checking data types before scaling...\")\n",
    "print(X_train.dtypes)\n",
    "\n",
    "# Identify non-numeric columns that need special handling\n",
    "non_numeric_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"Non-numeric columns: {non_numeric_cols}\")\n",
    "print(f\"Numeric columns: {numeric_cols}\")\n",
    "\n",
    "# Handle non-numeric columns (like IP addresses, categorical data)\n",
    "if non_numeric_cols:\n",
    "    print(\"Processing non-numeric columns...\")\n",
    "    \n",
    "    # For IP addresses: convert to numeric representation\n",
    "    if 'ip_add' in non_numeric_cols:\n",
    "        print(\"Converting IP addresses to numeric format...\")\n",
    "        \n",
    "        def ip_to_numeric(ip):\n",
    "            try:\n",
    "                if isinstance(ip, str) and '.' in ip:\n",
    "                    # Convert IP to numeric representation\n",
    "                    parts = ip.split('.')\n",
    "                    if len(parts) == 4:\n",
    "                        return (int(parts[0]) * 256**3 + int(parts[1]) * 256**2 + \n",
    "                                int(parts[2]) * 256 + int(parts[3]))\n",
    "                return 0\n",
    "            except:\n",
    "                return 0\n",
    "        \n",
    "        X_train['ip_add_numeric'] = X_train['ip_add'].apply(ip_to_numeric)\n",
    "        X_test['ip_add_numeric'] = X_test['ip_add'].apply(ip_to_numeric)\n",
    "        numeric_cols.append('ip_add_numeric')\n",
    "    \n",
    "    # For other categorical columns, use one-hot encoding or label encoding\n",
    "    categorical_cols = [col for col in non_numeric_cols if col != 'ip_add']\n",
    "    \n",
    "    if categorical_cols:\n",
    "        print(f\"Encoding categorical columns: {categorical_cols}\")\n",
    "        \n",
    "        # Use label encoding for simplicity (or one-hot encoding if few categories)\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            # Check if it's worth encoding (not too many unique values)\n",
    "            unique_vals = X_train[col].nunique()\n",
    "            if unique_vals < 50:  # Only encode if reasonable number of categories\n",
    "                le = LabelEncoder()\n",
    "                # Fit on training data and transform both train and test\n",
    "                X_train[col + '_encoded'] = le.fit_transform(X_train[col].astype(str))\n",
    "                X_test[col + '_encoded'] = le.transform(X_test[col].astype(str))\n",
    "                numeric_cols.append(col + '_encoded')\n",
    "                print(f\"  Encoded {col} with {unique_vals} categories\")\n",
    "            else:\n",
    "                print(f\"  Skipping {col} (too many categories: {unique_vals})\")\n",
    "\n",
    "# Now use only numeric columns for scaling\n",
    "print(f\"Final numeric columns for scaling: {numeric_cols}\")\n",
    "\n",
    "# Scale only the numeric features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled_numeric = scaler.fit_transform(X_train[numeric_cols])\n",
    "X_test_scaled_numeric = scaler.transform(X_test[numeric_cols])\n",
    "\n",
    "# Convert back to DataFrames\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled_numeric, columns=numeric_cols)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled_numeric, columns=numeric_cols)\n",
    "\n",
    "# Add target column\n",
    "if y_train.dtype == 'object':\n",
    "    y_train_binary = y_train.map({'good': 0, 'bad': 1}).fillna(0)\n",
    "    y_test_binary = y_test.map({'good': 0, 'bad': 1}).fillna(0)\n",
    "else:\n",
    "    y_train_binary = y_train\n",
    "    y_test_binary = y_test\n",
    "\n",
    "X_train_scaled_df['target'] = y_train_binary.values\n",
    "X_test_scaled_df['target'] = y_test_binary.values\n",
    "\n",
    "print(f\"Final training set shape: {X_train_scaled_df.shape}\")\n",
    "print(f\"Final test set shape: {X_test_scaled_df.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c71acac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Pre-encoded training dataset saved as 'quantum_phishing_features_train.csv'\n",
      "💾 Pre-encoded test dataset saved as 'quantum_phishing_features_test.csv'\n"
     ]
    }
   ],
   "source": [
    "# CELL 9: Save the pre-encoded datasets as CSV\n",
    "train_output_path = \"quantum_phishing_features_train.csv\"\n",
    "test_output_path = \"quantum_phishing_features_test.csv\"\n",
    "\n",
    "X_train_scaled_df.to_csv(train_output_path, index=False)\n",
    "X_test_scaled_df.to_csv(test_output_path, index=False)\n",
    "\n",
    "print(f\"💾 Pre-encoded training dataset saved as '{train_output_path}'\")\n",
    "print(f\"💾 Pre-encoded test dataset saved as '{test_output_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c56bfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available features: 15\n",
      "Feature info saved to 'feature_info.csv'\n",
      "✅ Feature extraction completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# CELL 10: Save feature list for reference (FIXED)\n",
    "print(f\"Number of available features: {len(available_features)}\")\n",
    "\n",
    "# Create a mapping of feature types based on the feature names\n",
    "feature_types = []\n",
    "for feature in available_features:\n",
    "    if feature in ['url_len', 'ip_add', 'geo_loc', 'tld', 'who_is', 'https', 'js_len', 'js_obf_len']:\n",
    "        feature_types.append('original')\n",
    "    elif any(keyword in feature for keyword in ['path', 'query', 'fragment', 'num_', 'url_entropy']):\n",
    "        feature_types.append('enhanced_url')\n",
    "    elif any(keyword in feature for keyword in ['html', 'script', 'iframe', 'link', 'form', 'image', 'input', 'style', 'external', 'suspicious']):\n",
    "        feature_types.append('enhanced_html')\n",
    "    elif any(keyword in feature for keyword in ['entanglement', 'superposition', 'prob_malicious', 'has_suspicious', 'quantum', 'entropy_ratio']):\n",
    "        feature_types.append('quantum')\n",
    "    else:\n",
    "        feature_types.append('other')\n",
    "\n",
    "feature_info_df = pd.DataFrame({\n",
    "    'feature_name': available_features,\n",
    "    'feature_type': feature_types\n",
    "})\n",
    "\n",
    "feature_info_df.to_csv(\"feature_info.csv\", index=False)\n",
    "print(\"Feature info saved to 'feature_info.csv'\")\n",
    "\n",
    "print(\"✅ Feature extraction completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1dc025b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current columns in your dataset:\n",
      "0: url\n",
      "1: url_len\n",
      "2: ip_add\n",
      "3: geo_loc\n",
      "4: tld\n",
      "5: who_is\n",
      "6: https\n",
      "7: js_len\n",
      "8: js_obf_len\n",
      "9: content\n",
      "10: target\n",
      "11: path_length\n",
      "12: query_length\n",
      "13: fragment_length\n",
      "14: num_dots\n",
      "15: num_hyphens\n",
      "16: num_underscores\n",
      "17: num_slashes\n",
      "18: num_questionmarks\n",
      "19: num_equals\n",
      "20: num_ampersands\n",
      "21: num_at_signs\n",
      "22: num_percent_signs\n",
      "23: num_digits\n",
      "24: num_letters\n",
      "25: num_special_chars\n",
      "26: url_entropy\n",
      "27: html_content_length\n",
      "28: num_script_tags\n",
      "29: num_iframe_tags\n",
      "30: num_link_tags\n",
      "31: num_form_tags\n",
      "32: num_image_tags\n",
      "33: num_input_tags\n",
      "34: num_style_tags\n",
      "35: num_external_links\n",
      "36: num_suspicious_keywords\n",
      "37: html_entropy\n",
      "38: script_ratio\n",
      "39: iframe_ratio\n",
      "40: external_link_ratio\n",
      "41: entanglement_url_html\n",
      "42: superposition_structure\n",
      "43: prob_malicious_url\n",
      "44: prob_malicious_html\n",
      "45: has_suspicious_elements\n",
      "46: js_quantum_ratio\n",
      "47: js_entropy_ratio\n",
      "\n",
      "Number of columns: 48\n",
      "\n",
      "Columns after processing:\n",
      "0: url\n",
      "1: url_len\n",
      "2: ip_add\n",
      "3: geo_loc\n",
      "4: tld\n",
      "5: who_is\n",
      "6: https\n",
      "7: js_len\n",
      "8: js_obf_len\n",
      "9: content\n",
      "10: target\n",
      "11: path_length\n",
      "12: query_length\n",
      "13: fragment_length\n",
      "14: num_dots\n",
      "15: num_hyphens\n",
      "16: num_underscores\n",
      "17: num_slashes\n",
      "18: num_questionmarks\n",
      "19: num_equals\n",
      "20: num_ampersands\n",
      "21: num_at_signs\n",
      "22: num_percent_signs\n",
      "23: num_digits\n",
      "24: num_letters\n",
      "25: num_special_chars\n",
      "26: url_entropy\n",
      "27: html_content_length\n",
      "28: num_script_tags\n",
      "29: num_iframe_tags\n",
      "30: num_link_tags\n",
      "31: num_form_tags\n",
      "32: num_image_tags\n",
      "33: num_input_tags\n",
      "34: num_style_tags\n",
      "35: num_external_links\n",
      "36: num_suspicious_keywords\n",
      "37: html_entropy\n",
      "38: script_ratio\n",
      "39: iframe_ratio\n",
      "40: external_link_ratio\n",
      "41: entanglement_url_html\n",
      "42: superposition_structure\n",
      "43: prob_malicious_url\n",
      "44: prob_malicious_html\n",
      "45: has_suspicious_elements\n",
      "46: js_quantum_ratio\n",
      "47: js_entropy_ratio\n",
      "\n",
      "Available important columns: ['url', 'url_len', 'ip_add', 'geo_loc', 'tld', 'who_is', 'https', 'js_len', 'js_obf_len', 'content']\n",
      "\n",
      "Could not find key columns. Let's manually identify them:\n",
      "URL column: url\n",
      "Target column: target\n",
      "Target values: ['good' 'bad']\n",
      "HTML column: content\n",
      "\n",
      "Too many columns (48). Selecting only important features...\n",
      "Reduced to 18 columns: ['url', 'url_len', 'ip_add', 'geo_loc', 'tld', 'https', 'js_len', 'js_obf_len', 'content', 'target', 'url_entropy', 'html_content_length', 'num_script_tags', 'script_ratio', 'entanglement_url_html', 'prob_malicious_url', 'js_quantum_ratio', 'js_entropy_ratio']\n"
     ]
    }
   ],
   "source": [
    "# Check what columns you actually have\n",
    "print(\"Current columns in your dataset:\")\n",
    "for i, col in enumerate(train_df.columns):\n",
    "    print(f\"{i}: {col}\")\n",
    "\n",
    "print(f\"\\nNumber of columns: {len(train_df.columns)}\")\n",
    "\n",
    "# Check if the first column is an unnamed index\n",
    "if train_df.columns[0].startswith('Unnamed'):\n",
    "    print(f\"\\nFirst column '{train_df.columns[0]}' appears to be an index column\")\n",
    "    \n",
    "    # Check if we should remove it\n",
    "    if train_df.columns[0] == 'Unnamed: 0':\n",
    "        # Check if this is just sequential numbers (like an index)\n",
    "        if train_df.iloc[:, 0].dtype in [np.int64, np.float64]:\n",
    "            print(\"Removing unnamed index column...\")\n",
    "            train_df = train_df.iloc[:, 1:]  # Keep all columns except first\n",
    "            test_df = test_df.iloc[:, 1:]\n",
    "            print(f\"Columns after removing index: {len(train_df.columns)}\")\n",
    "\n",
    "# Now check what columns remain\n",
    "print(\"\\nColumns after processing:\")\n",
    "for i, col in enumerate(train_df.columns):\n",
    "    print(f\"{i}: {col}\")\n",
    "\n",
    "# Let's see if we can identify the important columns\n",
    "important_columns = ['url', 'url_len', 'ip_add', 'geo_loc', 'tld', 'who_is', 'https', 'js_len', 'js_obf_len', 'content', 'label']\n",
    "\n",
    "# Find which of these important columns exist in your dataset\n",
    "available_important_cols = []\n",
    "for col in important_columns:\n",
    "    if col in train_df.columns:\n",
    "        available_important_cols.append(col)\n",
    "    else:\n",
    "        # Try to find similar columns\n",
    "        for actual_col in train_df.columns:\n",
    "            if col in actual_col.lower() or actual_col.lower() in col:\n",
    "                print(f\"Found similar: '{actual_col}' -> '{col}'\")\n",
    "                available_important_cols.append(actual_col)\n",
    "                break\n",
    "\n",
    "print(f\"\\nAvailable important columns: {available_important_cols}\")\n",
    "\n",
    "# If we found the key columns, use them\n",
    "if 'url' in available_important_cols and 'label' in available_important_cols:\n",
    "    url_col = 'url'\n",
    "    html_col = 'content' if 'content' in available_important_cols else None\n",
    "    target_col = 'label'\n",
    "    \n",
    "    print(f\"\\nUsing: URL='{url_col}', HTML='{html_col}', Target='{target_col}'\")\n",
    "    \n",
    "    # Verify the target column\n",
    "    print(f\"Target values: {train_df[target_col].unique()}\")\n",
    "    print(f\"Target counts:\\n{train_df[target_col].value_counts()}\")\n",
    "else:\n",
    "    print(\"\\nCould not find key columns. Let's manually identify them:\")\n",
    "    \n",
    "    # Look for URL column\n",
    "    url_candidates = [col for col in train_df.columns if 'url' in col.lower()]\n",
    "    if url_candidates:\n",
    "        url_col = url_candidates[0]\n",
    "        print(f\"URL column: {url_col}\")\n",
    "    \n",
    "    # Look for target/label column\n",
    "    target_candidates = [col for col in train_df.columns if any(x in col.lower() for x in ['label', 'target', 'class', 'type'])]\n",
    "    if target_candidates:\n",
    "        target_col = target_candidates[0]\n",
    "        print(f\"Target column: {target_col}\")\n",
    "        print(f\"Target values: {train_df[target_col].unique()}\")\n",
    "    \n",
    "    # Look for content/HTML column  \n",
    "    html_candidates = [col for col in train_df.columns if any(x in col.lower() for x in ['content', 'html', 'text', 'body'])]\n",
    "    if html_candidates:\n",
    "        html_col = html_candidates[0]\n",
    "        print(f\"HTML column: {html_col}\")\n",
    "\n",
    "# If you have many extra columns, let's select only the important ones\n",
    "if len(train_df.columns) > 20:  # If you have too many columns\n",
    "    print(f\"\\nToo many columns ({len(train_df.columns)}). Selecting only important features...\")\n",
    "    \n",
    "    # Keep only columns that are in our important list or contain key words\n",
    "    columns_to_keep = []\n",
    "    for col in train_df.columns:\n",
    "        if any(keyword in col.lower() for keyword in ['url', 'ip', 'geo', 'tld', 'whois', 'https', 'js', 'content', 'label', 'target', 'class']):\n",
    "            columns_to_keep.append(col)\n",
    "    \n",
    "    if columns_to_keep:\n",
    "        train_df = train_df[columns_to_keep]\n",
    "        test_df = test_df[columns_to_keep]\n",
    "        print(f\"Reduced to {len(columns_to_keep)} columns: {columns_to_keep}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab48f7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Quantum Dataset Generator - Feature Extraction\n",
    "# ========================================\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"📦 All libraries imported successfully!\")\n",
    "\n",
    "# Set paths to your dataset files\n",
    "train_data_path = \"train_data.csv\"\n",
    "test_data_path = \"test_data.csv\"\n",
    "\n",
    "# CELL 1: Load and explore the datasets\n",
    "print(\"Loading datasets...\")\n",
    "train_df = pd.read_csv(train_data_path)\n",
    "test_df = pd.read_csv(test_data_path)\n",
    "\n",
    "print(f\"Training set shape: {train_df.shape}\")\n",
    "print(f\"Test set shape: {test_df.shape}\")\n",
    "\n",
    "print(\"\\nTraining set columns:\")\n",
    "print(train_df.columns.tolist())\n",
    "print(\"\\nFirst few rows of training data:\")\n",
    "print(train_df.head(3))\n",
    "\n",
    "# CELL 2: Check what columns we actually have\n",
    "print(\"\\nAvailable columns in training data:\")\n",
    "for i, col in enumerate(train_df.columns):\n",
    "    print(f\"{i}: {col}\")\n",
    "\n",
    "# Based on your data sample, the columns are:\n",
    "# ,url,url_len,ip_add,geo_loc,tld,who_is,https,js_len,js_obf_len,content,label\n",
    "\n",
    "# Let's use the exact column names from your dataset\n",
    "url_col = 'url'\n",
    "html_col = 'content'\n",
    "target_col = 'label'\n",
    "\n",
    "print(f\"\\nUsing columns: URL='{url_col}', HTML='{html_col}', Target='{target_col}'\")\n",
    "\n",
    "# CELL 3: Basic preprocessing\n",
    "print(\"Checking for missing values...\")\n",
    "print(\"Missing values in training set:\")\n",
    "print(train_df.isnull().sum())\n",
    "print(\"Missing values in test set:\")\n",
    "print(test_df.isnull().sum())\n",
    "\n",
    "# Drop rows with missing values if any\n",
    "train_df = train_df.dropna()\n",
    "test_df = test_df.dropna()\n",
    "\n",
    "print(f\"Training set shape after cleaning: {train_df.shape}\")\n",
    "print(f\"Test set shape after cleaning: {test_df.shape}\")\n",
    "\n",
    "# CELL 4: Enhanced URL-based feature engineering\n",
    "def extract_url_features(url):\n",
    "    \"\"\"Extract additional features from URL\"\"\"\n",
    "    try:\n",
    "        if not isinstance(url, str):\n",
    "            url = str(url)\n",
    "            \n",
    "        parsed = urlparse(url)\n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        # Additional length features beyond the existing url_len\n",
    "        features['path_length'] = len(parsed.path) if parsed.path else 0\n",
    "        features['query_length'] = len(parsed.query) if parsed.query else 0\n",
    "        features['fragment_length'] = len(parsed.fragment) if parsed.fragment else 0\n",
    "        \n",
    "        # Structural features\n",
    "        features['num_dots'] = url.count('.')\n",
    "        features['num_hyphens'] = url.count('-')\n",
    "        features['num_underscores'] = url.count('_')\n",
    "        features['num_slashes'] = url.count('/')\n",
    "        features['num_questionmarks'] = url.count('?')\n",
    "        features['num_equals'] = url.count('=')\n",
    "        features['num_ampersands'] = url.count('&')\n",
    "        features['num_at_signs'] = url.count('@')\n",
    "        features['num_percent_signs'] = url.count('%')\n",
    "        \n",
    "        # Character composition\n",
    "        features['num_digits'] = sum(1 for c in url if c.isdigit())\n",
    "        features['num_letters'] = sum(1 for c in url if c.isalpha())\n",
    "        features['num_special_chars'] = len(url) - features['num_digits'] - features['num_letters']\n",
    "        \n",
    "        # Entropy features (quantum-inspired)\n",
    "        features['url_entropy'] = calculate_entropy(url)\n",
    "        \n",
    "        return features\n",
    "    except:\n",
    "        # Return default values if URL parsing fails\n",
    "        return {feature: 0 for feature in [\n",
    "            'path_length', 'query_length', 'fragment_length',\n",
    "            'num_dots', 'num_hyphens', 'num_underscores', 'num_slashes',\n",
    "            'num_questionmarks', 'num_equals', 'num_ampersands', \n",
    "            'num_at_signs', 'num_percent_signs', 'num_digits',\n",
    "            'num_letters', 'num_special_chars', 'url_entropy'\n",
    "        ]}\n",
    "\n",
    "def calculate_entropy(text):\n",
    "    \"\"\"Calculate Shannon entropy of a string efficiently\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return 0\n",
    "        \n",
    "    text_length = len(text)\n",
    "    if text_length == 0:\n",
    "        return 0\n",
    "        \n",
    "    # Use numpy for faster entropy calculation\n",
    "    chars, counts = np.unique(list(text), return_counts=True)\n",
    "    probabilities = counts / text_length\n",
    "    entropy = -np.sum(probabilities * np.log2(probabilities + 1e-10))  # Avoid log(0)\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "# Apply URL feature extraction with progress bar\n",
    "print(\"Extracting additional URL features from training set...\")\n",
    "url_features_train = []\n",
    "for url in tqdm(train_df[url_col], desc=\"Processing URLs\"):\n",
    "    url_features_train.append(extract_url_features(url))\n",
    "\n",
    "url_features_train = pd.DataFrame(url_features_train)\n",
    "train_df = pd.concat([train_df.reset_index(drop=True), url_features_train], axis=1)\n",
    "\n",
    "print(\"Extracting additional URL features from test set...\")\n",
    "url_features_test = []\n",
    "for url in tqdm(test_df[url_col], desc=\"Processing URLs\"):\n",
    "    url_features_test.append(extract_url_features(url))\n",
    "\n",
    "url_features_test = pd.DataFrame(url_features_test)\n",
    "test_df = pd.concat([test_df.reset_index(drop=True), url_features_test], axis=1)\n",
    "\n",
    "print(f\"Added {len(url_features_train.columns)} additional URL-based features\")\n",
    "\n",
    "# CELL 5: Enhanced HTML content features\n",
    "def extract_html_features(html_content):\n",
    "    \"\"\"Extract quantum-inspired features from HTML content\"\"\"\n",
    "    if not isinstance(html_content, str):\n",
    "        html_content = str(html_content)\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # Basic HTML structure features\n",
    "    html_length = len(html_content)\n",
    "    features['html_content_length'] = html_length\n",
    "    \n",
    "    # Only process HTML if it's not too long\n",
    "    if html_length < 100000:  # Reasonable limit\n",
    "        features['num_script_tags'] = html_content.count('<script')\n",
    "        features['num_iframe_tags'] = html_content.count('<iframe')\n",
    "        features['num_link_tags'] = html_content.count('<a')\n",
    "        features['num_form_tags'] = html_content.count('<form')\n",
    "        features['num_image_tags'] = html_content.count('<img')\n",
    "        features['num_input_tags'] = html_content.count('<input')\n",
    "        features['num_style_tags'] = html_content.count('<style')\n",
    "        \n",
    "        # Suspicious patterns\n",
    "        features['num_external_links'] = html_content.count('http://') + html_content.count('https://')\n",
    "        features['num_suspicious_keywords'] = sum(1 for keyword in ['eval', 'exec', 'document.write', 'innerHTML', 'fromCharCode'] \n",
    "                                               if keyword in html_content)\n",
    "        \n",
    "        # Quantum-inspired features\n",
    "        features['html_entropy'] = calculate_entropy(html_content) if html_length < 50000 else 0\n",
    "        \n",
    "        # Tag ratios\n",
    "        total_tags = html_content.count('<')\n",
    "        if total_tags > 0:\n",
    "            features['script_ratio'] = features['num_script_tags'] / total_tags\n",
    "            features['iframe_ratio'] = features['num_iframe_tags'] / total_tags\n",
    "            features['external_link_ratio'] = features['num_external_links'] / total_tags\n",
    "        else:\n",
    "            features['script_ratio'] = 0\n",
    "            features['iframe_ratio'] = 0\n",
    "            features['external_link_ratio'] = 0\n",
    "    else:\n",
    "        # Default values for very long HTML\n",
    "        features.update({\n",
    "            'num_script_tags': 0, 'num_iframe_tags': 0, 'num_link_tags': 0,\n",
    "            'num_form_tags': 0, 'num_image_tags': 0, 'num_input_tags': 0,\n",
    "            'num_style_tags': 0, 'num_external_links': 0, 'num_suspicious_keywords': 0,\n",
    "            'html_entropy': 0, 'script_ratio': 0, 'iframe_ratio': 0, 'external_link_ratio': 0\n",
    "        })\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Apply HTML feature extraction with progress bar\n",
    "print(\"Extracting HTML features from training set...\")\n",
    "html_features_train = []\n",
    "for html in tqdm(train_df[html_col], desc=\"Processing HTML\"):\n",
    "    html_features_train.append(extract_html_features(html))\n",
    "\n",
    "html_features_train = pd.DataFrame(html_features_train)\n",
    "train_df = pd.concat([train_df, html_features_train], axis=1)\n",
    "\n",
    "print(\"Extracting HTML features from test set...\")\n",
    "html_features_test = []\n",
    "for html in tqdm(test_df[html_col], desc=\"Processing HTML\"):\n",
    "    html_features_test.append(extract_html_features(html))\n",
    "\n",
    "html_features_test = pd.DataFrame(html_features_test)\n",
    "test_df = pd.concat([test_df, html_features_test], axis=1)\n",
    "\n",
    "print(f\"Added {len(html_features_train.columns)} HTML-based features\")\n",
    "\n",
    "# CELL 6: Create quantum-inspired features\n",
    "def create_quantum_features(row):\n",
    "    \"\"\"Create features inspired by quantum concepts\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Feature entanglement (correlation-inspired)\n",
    "    features['entanglement_url_html'] = row.get('url_entropy', 0) * row.get('html_entropy', 0)\n",
    "    \n",
    "    # Superposition-inspired features\n",
    "    html_len = row.get('html_content_length', 1)\n",
    "    features['superposition_structure'] = (row.get('num_script_tags', 0) + row.get('num_iframe_tags', 0)) / max(html_len, 1) * 100\n",
    "    \n",
    "    # Quantum probability-inspired features\n",
    "    url_len = max(row.get('url_len', 1), 1)\n",
    "    features['prob_malicious_url'] = min(1.0, row.get('num_special_chars', 0) / url_len * 5)\n",
    "    features['prob_malicious_html'] = min(1.0, (row.get('num_script_tags', 0) + row.get('num_iframe_tags', 0)) / max(html_len, 1) * 100)\n",
    "    \n",
    "    # Quantum state-inspired binary features\n",
    "    features['has_suspicious_elements'] = 1 if (row.get('num_script_tags', 0) > 3 or \n",
    "                                              row.get('num_iframe_tags', 0) > 2 or\n",
    "                                              row.get('num_suspicious_keywords', 0) > 5) else 0\n",
    "    \n",
    "    # JS-related quantum features\n",
    "    js_len = max(row.get('js_len', 1), 1)\n",
    "    js_obf_len = row.get('js_obf_len', 0)\n",
    "    features['js_quantum_ratio'] = js_obf_len / js_len\n",
    "    features['js_entropy_ratio'] = calculate_entropy(str(js_len)) / max(calculate_entropy(str(js_obf_len)), 1)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Apply quantum feature creation with progress bar\n",
    "print(\"Creating quantum-inspired features for training set...\")\n",
    "quantum_features_train = []\n",
    "for _, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"Quantum features\"):\n",
    "    quantum_features_train.append(create_quantum_features(row))\n",
    "\n",
    "quantum_features_train = pd.DataFrame(quantum_features_train)\n",
    "train_df = pd.concat([train_df, quantum_features_train], axis=1)\n",
    "\n",
    "print(\"Creating quantum-inspired features for test set...\")\n",
    "quantum_features_test = []\n",
    "for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Quantum features\"):\n",
    "    quantum_features_test.append(create_quantum_features(row))\n",
    "\n",
    "quantum_features_test = pd.DataFrame(quantum_features_test)\n",
    "test_df = pd.concat([test_df, quantum_features_test], axis=1)\n",
    "\n",
    "print(f\"Added {len(quantum_features_train.columns)} quantum-inspired features\")\n",
    "\n",
    "# CELL 7: Prepare the final dataset\n",
    "# Select features for the final dataset (using original column names)\n",
    "feature_columns = [\n",
    "    # Original features from your dataset\n",
    "    'url_len', 'ip_add', 'geo_loc', 'tld', 'who_is', 'https', \n",
    "    'js_len', 'js_obf_len',\n",
    "    \n",
    "    # Enhanced URL features\n",
    "    'path_length', 'query_length', 'fragment_length',\n",
    "    'num_dots', 'num_hyphens', 'num_underscores', 'num_slashes',\n",
    "    'num_questionmarks', 'num_equals', 'num_ampersands', \n",
    "    'num_at_signs', 'num_percent_signs', 'num_digits',\n",
    "    'num_letters', 'num_special_chars', 'url_entropy',\n",
    "    \n",
    "    # Enhanced HTML features\n",
    "    'html_content_length', 'num_script_tags', 'num_iframe_tags', \n",
    "    'num_link_tags', 'num_form_tags', 'num_image_tags',\n",
    "    'num_input_tags', 'num_style_tags', 'num_external_links',\n",
    "    'num_suspicious_keywords', 'html_entropy', 'script_ratio',\n",
    "    'iframe_ratio', 'external_link_ratio',\n",
    "    \n",
    "    # Quantum features\n",
    "    'entanglement_url_html', 'superposition_structure',\n",
    "    'prob_malicious_url', 'prob_malicious_html', 'has_suspicious_elements',\n",
    "    'js_quantum_ratio', 'js_entropy_ratio'\n",
    "]\n",
    "\n",
    "# Keep only features that actually exist in the dataframe\n",
    "available_features = [f for f in feature_columns if f in train_df.columns]\n",
    "print(f\"Using {len(available_features)} available features\")\n",
    "\n",
    "# Create the feature datasets\n",
    "X_train = train_df[available_features]\n",
    "y_train = train_df[target_col]\n",
    "X_test = test_df[available_features]\n",
    "y_test = test_df[target_col]\n",
    "\n",
    "print(f\"Training feature matrix shape: {X_train.shape}\")\n",
    "print(f\"Test feature matrix shape: {X_test.shape}\")\n",
    "\n",
    "# CELL 8: Data normalization\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrames\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=available_features)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=available_features)\n",
    "\n",
    "# Add target column (convert to binary if needed)\n",
    "# Assuming 'good' = 0, 'bad' = 1 or similar\n",
    "if y_train.dtype == 'object':\n",
    "    y_train_binary = y_train.map({'good': 0, 'bad': 1}).fillna(0)\n",
    "    y_test_binary = y_test.map({'good': 0, 'bad': 1}).fillna(0)\n",
    "else:\n",
    "    y_train_binary = y_train\n",
    "    y_test_binary = y_test\n",
    "\n",
    "X_train_scaled_df['target'] = y_train_binary.values\n",
    "X_test_scaled_df['target'] = y_test_binary.values\n",
    "\n",
    "# CELL 9: Save the pre-encoded datasets as CSV\n",
    "train_output_path = \"quantum_phishing_features_train.csv\"\n",
    "test_output_path = \"quantum_phishing_features_test.csv\"\n",
    "\n",
    "X_train_scaled_df.to_csv(train_output_path, index=False)\n",
    "X_test_scaled_df.to_csv(test_output_path, index=False)\n",
    "\n",
    "print(f\"💾 Pre-encoded training dataset saved as '{train_output_path}'\")\n",
    "print(f\"💾 Pre-encoded test dataset saved as '{test_output_path}'\")\n",
    "\n",
    "# CELL 10: Save feature list for reference\n",
    "feature_info_df = pd.DataFrame({\n",
    "    'feature_name': available_features,\n",
    "    'feature_type': ['original']*8 + ['enhanced_url']*16 + ['enhanced_html']*14 + ['quantum']*7\n",
    "})\n",
    "feature_info_df.to_csv(\"feature_info.csv\", index=False)\n",
    "\n",
    "print(\"✅ Feature extraction completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
